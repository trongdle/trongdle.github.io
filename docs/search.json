[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog I do not understand"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Preparation for Dr. Gebru’s talk at Middlebury College.\n\n\n\n\n\n\nApr 18, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThe Adam optimizer and some simple experiments.\n\n\n\n\n\n\nApr 17, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nLinear algebra methods for unsupervised learning with two kinds of data: images and graphs\n\n\n\n\n\n\nApr 13, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog post, I will create a machine learning model that predicts an individual characteristic like employment status or income on the basis of other demographic characteristics.\n\n\n\n\n\n\nMar 25, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog post, I will implement least-squares linear regression, and experiment with LASSO regularization for overparameterized problems.\n\n\n\n\n\n\nMar 23, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nClassifying Palmer Penguins with Sklearn.\n\n\n\n\n\n\nMar 7, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nOptimization algorithms based on the gradients of functions.\n\n\n\n\n\n\nFeb 26, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog post, I will implement the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets.\n\n\n\n\n\n\nFeb 19, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/adam/adam.html",
    "href": "posts/adam/adam.html",
    "title": "Optimization with Adam",
    "section": "",
    "text": "A. Stochastic Gradient Descent vs. Adam Optimization\nWe will use the data given below to compare between stochastic logistic regression and Adam optimization.\n\nfrom solutions import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .01)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit_adam(X, y, batch_size=10, alpha=0.001, beta1=0.9, beta2=0.999, w_0 = None, epsilon=1e-8, m_epochs=10000)\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Adam gradient\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Empirical Risk\")\nlegend = plt.legend() \n\n\n\n\nThe learning rate for the stochastic gradient method is 10 times bigger than that for the Adam method, but the Adam method converges in half the time steps!\n\n\nB. Digits Experiment\nWe will use the digits dataset from sklearn. I will filter the digits data set so that it only contains data with two class labels, namely 0s and 1s.\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndigits = load_digits()\n\n# Filter the data to only contain 4s and 8s\nX = digits.data[(digits.target == 0) | (digits.target == 1)]\ny = digits.target[(digits.target == 0) | (digits.target == 1)]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nLR_momentum = LogisticRegression()\n\nLR_momentum.fit_stochastic(X_train/255, y_train, \n            m_epochs = 10000, \n            momentum = True, \n            batch_size = 10, \n            alpha = .1)\nnum_steps = len(LR_momentum.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_momentum.loss_history, label = \"accelerated stochastic gradient\")\n\nLR_Adam = LogisticRegression()\nLR_Adam.fit_adam(X_train/255, y_train, batch_size=10, alpha=0.001, beta1=0.9, beta2=0.999, w_0 = None, epsilon=1e-8, m_epochs=10000)\nnum_steps = len(LR_Adam.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_Adam.loss_history, label = \"Adam\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Empirical Risk\")\nlegend = plt.legend() \n\n\n\n\nSo it seems that the Adam optimization is much faster than the stochastic gradient with momentum method! And now we will compare their efficiencies on unseen data.\n\nprint('The score of the Adam machine is ' + str(LR_Adam.score(X_test/255, y_test)) + '.')\nprint('The score of the accelerated stochastic gradient method is ' + str(LR_momentum.score(X_test/255, y_test)) + '.')\nprint('The loss of the Adam machine is ' + str(LR_Adam.loss(X_test/255, y_test)) + '.')\nprint('The loss of the accelerated stochastic gradient method is ' + str(LR_momentum.loss(X_test/255, y_test)) + '.')\n\nThe score of the Adam machine is 1.0.\nThe score of the accelerated stochastic gradient method is 1.0.\nThe loss of the Adam machine is 0.49792824663572666.\nThe loss of the accelerated stochastic gradient method is 0.49792824663572666.\n\n\nThese numbers look the same, but the Adam machine achieved these numbers in a much faster fashion.\n\n\nOne More Experiment\nI will do one more experiment on a dataset that looks like the one in section A, except that there are 80 features.\n\n# make the data\np_features = 81\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .01)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit_adam(X, y, batch_size=10, alpha=0.001, beta1=0.9, beta2=0.999, w_0 = None, epsilon=1e-8, m_epochs=10000)\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Adam gradient\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Empirical Risk\")\nlegend = plt.legend() \n\n\n\n\nThe Adam algorithm again comes out on top!"
  },
  {
    "objectID": "posts/audit/audit.html",
    "href": "posts/audit/audit.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "from folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"TX\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000026\n      7\n      1\n      5000\n      3\n      48\n      1013097\n      29\n      21\n      ...\n      29\n      4\n      5\n      54\n      27\n      52\n      28\n      29\n      52\n      29\n    \n    \n      1\n      P\n      2018GQ0000057\n      7\n      1\n      6601\n      3\n      48\n      1013097\n      16\n      19\n      ...\n      33\n      3\n      18\n      16\n      32\n      3\n      18\n      16\n      2\n      2\n    \n    \n      2\n      P\n      2018GQ0000070\n      7\n      1\n      4302\n      3\n      48\n      1013097\n      64\n      24\n      ...\n      14\n      64\n      110\n      62\n      14\n      15\n      64\n      64\n      13\n      67\n    \n    \n      3\n      P\n      2018GQ0000079\n      7\n      1\n      700\n      3\n      48\n      1013097\n      260\n      20\n      ...\n      57\n      451\n      261\n      272\n      59\n      477\n      261\n      258\n      480\n      56\n    \n    \n      4\n      P\n      2018GQ0000082\n      7\n      1\n      900\n      3\n      48\n      1013097\n      12\n      31\n      ...\n      10\n      3\n      11\n      22\n      12\n      20\n      1\n      12\n      21\n      1\n    \n  \n\n5 rows × 286 columns\n\n\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      21\n      16.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      2\n      1.0\n    \n    \n      1\n      19\n      16.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n    \n    \n      2\n      24\n      12.0\n      5\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      1.0\n      1\n      2\n      6.0\n    \n    \n      3\n      20\n      16.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      3.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      1.0\n    \n    \n      4\n      31\n      17.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      4\n      1\n      2\n      2\n      2.0\n      1\n      1\n      1.0\n    \n  \n\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\nprint('1. There are ' + str(len(df)) + ' individuals in this survey.')\nprint('2. Of those individuals, ' + str(len(df[df['label'] == 1])) + ' are employed.')\n\n1. There are 214480 individuals in this survey.\n2. Of those individuals, 96805 are employed.\n\n\n\nprint('3. The number of individuals in each racial group is: \\n' + str(df.groupby('group')['label'].aggregate(len)))\n\n3. The number of individuals in each racial group is: \ngroup\n1    165969\n2     20614\n3       836\n4        14\n5       401\n6     10141\n7       158\n8     10544\n9      5803\nName: label, dtype: int64\n\n\n\nprint('4. The proportion of employed individuals in each racial group is: \\n' + str(df[df['label'] == 1].groupby('group')['label'].aggregate(len)/df.groupby('group')['label'].aggregate(len)))\n\n4. The proportion of employed individuals in each racial group is: \ngroup\n1    0.455242\n2    0.416756\n3    0.431818\n4    0.357143\n5    0.461347\n6    0.499359\n7    0.430380\n8    0.466711\n9    0.353955\nName: label, dtype: float64\n\n\n\nprint('5.1. The proportion of employed male individuals in each racial group is: \\n' + str(df[df['SEX'] == 1][df['label'] == 1].groupby('group')['label'].aggregate(len)/df[df['SEX'] == 1].groupby('group')['label'].aggregate(len)))\n\n5.1. The proportion of employed male individuals in each racial group is: \ngroup\n1    0.502327\n2    0.396673\n3    0.474178\n4    0.250000\n5    0.488263\n6    0.553741\n7    0.428571\n8    0.529160\n9    0.371400\nName: label, dtype: float64\n\n\nC:\\Users\\ledtr\\AppData\\Local\\Temp\\ipykernel_5008\\3156142459.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  print('5.1. The proportion of employed male individuals in each racial group is: \\n' + str(df[df['SEX'] == 1][df['label'] == 1].groupby('group')['label'].aggregate(len)/df[df['SEX'] == 1].groupby('group')['label'].aggregate(len)))\n\n\n\nprint('5.2. The proportion of employed female individuals in each racial group is: \\n' + str(df[df['SEX'] == 2][df['label'] == 1].groupby('group')['label'].aggregate(len)/df[df['SEX'] == 2].groupby('group')['label'].aggregate(len)))\n\n5.2. The proportion of employed female individuals in each racial group is: \ngroup\n1    0.410106\n2    0.435596\n3    0.387805\n4    0.500000\n5    0.430851\n6    0.447318\n7    0.432836\n8    0.401550\n9    0.335905\nName: label, dtype: float64\n\n\nC:\\Users\\ledtr\\AppData\\Local\\Temp\\ipykernel_5008\\266627361.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  print('5.2. The proportion of employed female individuals in each racial group is: \\n' + str(df[df['SEX'] == 2][df['label'] == 1].groupby('group')['label'].aggregate(len)/df[df['SEX'] == 2].groupby('group')['label'].aggregate(len)))\n\n\nExcept for groups 2, 4 and 7, the other racial groups have a higher proportion of employed men than women.\nNow we will train our model. I’m using the DecisionTreeClassifier model. Below is my code to see which maximum depth of the tree gives the most accuracy. The cross-validation is only done on the training data.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\ncross_train, cross_test, crossy_train, crossy_test = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=0)\nscore_i = []\n\nfor i in range(1, 30):\n    model = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth = i))\n    model.fit(cross_train, crossy_train)\n    y_hat = model.predict(cross_test)\n    score = (y_hat == crossy_test).mean()\n    score_i.append(score)\n\nplt.plot(np.arange(1, 30), score_i)\nplt.xlabel('max_depth')\nplt.ylabel('score')\nplt.grid()\n\n\n\n\nIt seems that max_depth = 9 is our optimal tree depth. We will now test this model on test data.\n\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth = 9))\nmodel.fit(X_train, y_train)\ny_hat = model.predict(X_test)\nscore = (y_hat == y_test).mean()\nprint('1. The overall accuracy of the model is ' + str(score.round(4)) + '.')\n\nfrom sklearn.metrics import confusion_matrix\nC = confusion_matrix(y_test, y_hat, normalize='true')\nTN = C[0][0]\nFP = C[0][1]\nFN = C[1][0]\nTP = C[1][1]\nPPV = TP/(TP+FP)\nprint('2. The overall positive predictive value is ' + str(PPV.round(4)) + '.')\nprint('3. The overall false negative rate is ' + str(FN.round(4)) + ' and false positive rate is ' + str(FP.round(4)) + '.')\n\n1. The overall accuracy of the model is 0.8237.\n2. The overall positive predictive value is 0.8045.\n3. The overall false negative rate is 0.1361 and false positive rate is 0.2099.\n\n\n\nsub_acc = []\nsub_PPV = []\nsub_FNR = []\nsub_FPR = []\n\nfor i in range(1,10):\n    y_hat = model.predict(X_test)\n    score = (y_hat[group_test == i] == y_test[group_test == i]).mean()\n    sub_acc.append(score)\n    C = confusion_matrix(y_test[group_test == i], y_hat[group_test == i], normalize='true')\n    TN = C[0][0]\n    FP = C[0][1]\n    FN = C[1][0]\n    TP = C[1][1]\n    PPV = TP/(TP+FP)\n    sub_PPV.append(PPV)\n    sub_FNR.append(FN)\n    sub_FPR.append(FP)\n\nNow we will look at these numbers in each subgroup. The information is displayed in the following graphs.\n\nfig1, ax1 = plt.subplots()\nhello = np.arange(1,10)\nax1.plot(hello, sub_acc)\nax1.set_xlabel('group')\nax1.set_ylabel('score')\nax1.set_title('Subgroup accuracy')\nax1.grid()\n\nfig2, ax2 = plt.subplots()\nax2.plot(hello, sub_PPV)\nax2.set_xlabel('group')\nax2.set_ylabel('Positive predictive value')\nax2.set_title('Subgroup PPV')\nax2.grid()\n\nfig3, ax3 = plt.subplots()\nax3.plot(hello, sub_FNR, label = 'False Negative Rate')\nax3.plot(hello, sub_FPR, label = 'False Positive Rate')\nax3.set_xlabel('group')\nax3.set_ylabel('False rate')\nax3.set_title('Subgroup False Prediction Rate')\nax3.legend()\nax3.grid()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThis model is approximately calibrated. If we exclude group 4, which has a small sample size of 14 individuals, the positive predictive value for each group is close to the overall PPV. It is the same case for the FNR and FPR, which means that this model also satisfies approximate error rate balance. This means that our model has the same prevalence for all groups, which means that this is an amazing model!\n\nConclusion\n\nThis model can be used by job referrals websites such as LinkedIn to advertise to potentially unemployed people. It can also be used by car insurance companies to advertise to potentially employed people, who may own a car to commute to work. Or it can be used by apartment complexes to advertise to employed people.\nI think that if this model is used in a governmental setting, the model can predict the employability of a person based on their features excluding race. This can be used to decide whether a person is eligible for a loan (if they can pay back the loan by working at a job), whether they can rent a place in a certain area (expensive areas require a stable and supple income).\nBased on my audit, my model does not have any problematic bias.\nA problem with my model is that it cannot accurately predict for racial groups that have too few individuals."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "beta*(w_tilda - w_tilda_prev) # this is w_{k+1} w_tilda_prev = w_tilda # this is w_{k-1} w_tilda = w_tilda_new # this is w_{k}\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/gebru/gebru.html",
    "href": "posts/gebru/gebru.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "Introduction\nOn April 24, in class, we will have a chance to have a virtual Q&A session with Dr. Gebru about her recent work in AI ethics. In the evening of April 24th, at 7pm in Hillcrest 103, Dr. Gebru will give a talk on “Eugenics and the Promise of Utopia through Artificial General Intelligence.”\nDr. Gebru has gained international recognition due to her groundbreaking research and contributions to the field. Her work has been instrumental in highlighting the impact of AI on marginalized communities, exposing bias in machine learning, and emphasizing the importance of ethical considerations in the development of AI systems. Dr. Gebru’s expertise in the field is highly sought after, and her presence at Middlebury College is a significant event that is sure to provide unparalleled insights and inspire important conversations about the evils that artificial intelligence is capable of.\nDr. Gebru’s Talk\nIn 2020, Dr. Gebru gave a talk as part of a Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision at the conference on Computer Vision and Pattern Recognition 2020. The recording of her talk can be found here.\nDuring her talk, Dr. Gebru spoke at length about the intrusive and destructive use of artificial intelligence on marginalized groups. For example, facial recognition was used to match protestors to their social media to ensure arrests. Allegedly, with the help of robot police, nobody is safe from profiling. That robot police hate minorities as much as human police is a result of bias in datasets used to train computer vision systems. Dr. Gebru noted that many of these datasets were collected, trained, sold, and used by white people and corporations, which unavoidably led to ineffective predictive models for minorities. She argued that computer vision systems are not neutral technologies but are instead shaped by the social, cultural, and political contexts in which they are used. As such, it is important to consider how these systems may impact different communities and to ensure that they are designed and used in ways that are both equitable and just.\nAnother issue with these models is that even if a model works equally well on anyone (i.e. no bias), it can still be bad. For example, if the AI model predicts that a user is female, it will direct advertisements about things that women are encouraged to use, thus reinforcing existing stereotypes and discriminations. In a broader context, Dr. Gebru noted that by making a model ‘fairer’, we would only make the institutions that use the model colder and more punitive.\nDr. Gebru also argued that a ‘good’ model could still inflict harm on minorities if it falls into the wrong hands. This is because researchers in the field over-abstract their work, up to the point where humans in a study are disappeared into mathematical equations. It is important to consider that when working with data, we see subjects and objects, but on both sides are human beings. The most effective way to stop AI from inflicting harm is for scientists to actively prevent marginalization when they are working on their models.\ntl;dr\nThe primary takeaway from Dr. Gebru’s talk is that computer vision systems are not neutral technologies but are instead shaped by the social, cultural, and political contexts in which they are used, and as such, it is important to consider the potential biases and impact of these systems on marginalized communities.\nQuestions for Dr. Gebru\nIn Dr. Gebru’s view, what role should policymakers play in addressing issues of bias and fairness in the development and deployment of computer vision systems, and what steps can they take to ensure that these systems are designed and used in ways that are both equitable and just?"
  },
  {
    "objectID": "posts/gradient/gradient.html",
    "href": "posts/gradient/gradient.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "Below is our data, which is not linearly separable.\n\nfrom solutions import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef sigmoid(z):\n    # the sigmoid function\n    return 1 / (1 + np.exp(-z))\n\n\nn, p = X.shape\n\n# weight tilda and X tilda that has one extra term\nw_tilda = np.random.rand(p+1)\nX_tilda = np.append(X, np.ones((X.shape[0], 1)), 1)\n\ny_hat = np.dot(X_tilda, w_tilda) #inner product between X and w\ndeltaL = 1/n*(sigmoid(y_hat) - y)@X_tilda\ndeltaL\n\narray([-0.22233893, -0.1366533 ,  0.09319262])\n\n\n\n1. Gradient Descent\nThe gradient descent algorithm is implemented when the LogisticRegression.fit(X, y) is called. \\(\\textbf{X}\\) is the feature matrix, and \\(\\textbf{y}\\) is the target vector, as we know from the perceptron blog post. We need to find a weight vector that can minimize the logistic loss on \\(\\textbf{X}\\) and \\(\\textbf{y}\\) by updating the weight in a loop. At every loop:\n\ncompute the gradient descent of the logistic loss, which is given by \\[\\nabla L(\\textbf{w}) = \\frac{1}{n} \\sum^n_{i=1} (\\sigma (\\langle \\textbf{w}, \\textbf{x}_i \\rangle), y_i) \\textbf{w}_i, \\] where \\(\\sigma(z)\\) denotes the sigmoid function.\nupdate the weight until the logistic loss does not change \\[\\textbf{w}^{(t+1)} = \\textbf{w}^{(t)} - \\alpha \\nabla L(\\textbf{w}^{(t)})\\]\n\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 10, max_epochs = 1000)\n\n# inspect the fitted value of w\nLR.w \n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfigy, axy = plt.subplots()\naxy.scatter(X[:,0], X[:,1], c = y)\ndraw_line(LR.w, -2, 2)\naxy.set_xlabel(\"Feature 1\")\naxy.set_ylabel(\"Feature 2\")\n\nfigx, axv = plt.subplots()\naxv.plot(LR.loss_history)\naxv.set_xlabel(\"Iteration\")\naxv.set_ylabel(\"Empirical Risk\")\nplt.show()\n\n\n\n\n\n\n\nIn this case, the learning rate was set very high to 10, which caused the algorithm to converge too soon and leave the logistic loss at 0.24. As we change the learning rate to 0.1, we can see that the loss fell to 0.24.\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha=0.1, max_epochs=1000)\n\nfigy, axy = plt.subplots()\naxy.scatter(X[:,0], X[:,1], c = y)\ndraw_line(LR.w, -2, 2)\naxy.set_xlabel(\"Feature 1\")\naxy.set_ylabel(\"Feature 2\")\n\nfigx, axv = plt.subplots()\naxv.plot(LR.loss_history)\naxv.set_xlabel(\"Iteration\")\naxv.set_ylabel(\"Empirical Risk\")\nplt.show()\n\n\n\n\n\n\n\n\n\n2. Stochastic Gradient Descent\nThe stochastic gradient descent algorithm is implemented by the LogisticRegression.fit_stochastic(X, y) method. In this algorithm, instead of computing the complete gradient as in part 1, we compute a stochastic gradient by picking a random subset \\(S \\subseteq [n] = \\{1, ..., n\\}\\) and computing\n\\[\\nabla_S L(\\textbf{w}) = \\frac{1}{|S|} \\sum_{i \\in S} \\nabla (\\sigma (\\langle \\textbf{w}, \\textbf{x}_i \\rangle), y_i)\\textbf{x}_i \\]\nThe size of \\(S\\) is the batch size, which we can refer to as \\(k\\). The batch gradient descent is performed as follows: 1. Shuffle the points randomly. 2. Pick the first \\(k\\) random points, compute the stochastic gradient, and then perform an update. 3. Pick the next \\(k\\) random points and repeat.. 4. When we have gone through all \\(n\\) points, reshuffle them all randomly and proceed again.\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this experiment, we can see how the stochastic gradient descent algorithm can outperform the original gradient descent method.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .1, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Empirical Risk\")\nlegend = plt.legend() \n\n\n\n\nThe stochastic gradient method used only 150 loops, while the gradient method still had not converged at loop 1000.\n\n\n3. Stochastic Gradient Descent with Momentum\nThis only difference in this method comes from the momentum factor \\(\\beta\\). The weight update is given by\n\\[\\textbf{w}^{(t+1)} = \\textbf{w}^{(t)} - \\alpha \\nabla L(\\textbf{w}^{(t)}) + \\beta (\\textbf{w}^{(t)}-\\textbf{w}^{(t-1)})\\]\nThe idea here is that if the previous weight update was good, we may want to continue moving along this direction. To test how the momentum can help with accelerating convergence, we choose a small sample size of 150, distributed among 10 features.\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 11\nX, y = make_blobs(n_samples = 150, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\nlegend = plt.legend() \nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Empirical Risk\")\nplt.show()\n\n\n\n\nThe momentum algorithm converges much faster than the other method."
  },
  {
    "objectID": "posts/linear-regression/linear-regression.html",
    "href": "posts/linear-regression/linear-regression.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "The optimization for linear regression can be implemented by using my source code linear.py, which can be found HERE. In this code, loss optimization is implemented in two ways: using gradient descent, and using the optimal weight formula. We will check if this code is implemented correctly.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\nfrom linear import LinearRegression\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.6311\nValidation score = 0.6072\n\n\n\nLR.w.reshape(-1,1)\n\narray([[1.05653571],\n       [0.96877292]])\n\n\n\nLR2 = LinearRegression()\ny2 = y_train.reshape(-1,1)\nLR2.fit_gradient(X_train, y2, alpha=0.01)\nLR2.w\n\narray([[1.10993122],\n       [0.94029021]])\n\n\nSo the weights in both cases are very similar, which means that the gradient method is behaving as it should.\nThe score also increases monotonically in each iteration.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")"
  },
  {
    "objectID": "posts/linear-regression/linear-regression.html#experiments",
    "href": "posts/linear-regression/linear-regression.html#experiments",
    "title": "Implementing Linear Regression",
    "section": "Experiments",
    "text": "Experiments\nNow that we have established that our LinearRegression module is behaving okay, we can move on to experiments, where we increase the number of features and observe the change in validation scores.\n\nn_train = 100\nn_val = 100\np_features = np.arange(1,100)\nnoise = 0.2\ntrain_score = []\ntest_score = []\n\n# create some data\nfor i in range(len(p_features)):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features[i], noise)\n    LR = LinearRegression()\n    LR.fit_analytic(X_train, y_train)\n    train_score.append(LR.score(X_train, y_train).round(4))\n    test_score.append(LR.score(X_val, y_val).round(4))\n\nplt.plot(p_features, train_score, label = 'Training score')\nplt.plot(p_features, test_score, label = 'Validation score')\nplt.legend()\nplt.xlabel('Number of features')\nplt.ylabel('Score')\nplt.grid()\nplt.show()\n\n\n\n\nAs the number of features increases, the training score approaches 100% while validation score becomes noisier. This demonstrates the effect of overfitting: because the machine is trained to account for every little detail of the training dataset, it becomes too inflexible to work with unseen data."
  },
  {
    "objectID": "posts/linear-regression/linear-regression.html#lasso-regularization",
    "href": "posts/linear-regression/linear-regression.html#lasso-regularization",
    "title": "Implementing Linear Regression",
    "section": "LASSO Regularization",
    "text": "LASSO Regularization\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\n\nn_train = 100\nn_val = 100\np_features = np.arange(1,100)\nnoise = 0.2\ntrain_score = []\ntest_score = []\n\n# create some data\nfor i in range(len(p_features)):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features[i], noise)\n    L = Lasso(alpha = 0.001)\n    L.fit(X_train, y_train)\n    train_score.append(L.score(X_train, y_train).round(4))\n    test_score.append(L.score(X_val, y_val).round(4))\n\nplt.plot(p_features, train_score, label = 'Training score')\nplt.plot(p_features, test_score, label = 'Validation score')\nplt.legend()\nplt.xlabel('Number of features')\nplt.ylabel('Score')\nplt.grid()\nplt.show()\n\n\n\n\nIt seems that even when the number of features used is large, the validation score isn’t affected as badly as in the case of pure linear regression. Now we change the strength of regularization to see what changes.\n\ntrain_score = []\ntest_score = []\n\n# create some data\nfor i in range(len(p_features)):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features[i], noise)\n    L = Lasso(alpha = 0.01)\n    L.fit(X_train, y_train)\n    train_score.append(L.score(X_train, y_train).round(4))\n    test_score.append(L.score(X_val, y_val).round(4))\n\nplt.plot(p_features, train_score, label = 'Training score')\nplt.plot(p_features, test_score, label = 'Validation score')\nplt.legend()\nplt.xlabel('Number of features')\nplt.ylabel('Score')\nplt.grid()\nplt.show()\n\n\n\n\n\ntrain_score = []\ntest_score = []\n\n# create some data\nfor i in range(len(p_features)):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features[i], noise)\n    L = Lasso(alpha = 0.0005)\n    L.fit(X_train, y_train)\n    train_score.append(L.score(X_train, y_train).round(4))\n    test_score.append(L.score(X_val, y_val).round(4))\n\nplt.plot(p_features, train_score, label = 'Training score')\nplt.plot(p_features, test_score, label = 'Validation score')\nplt.legend()\nplt.xlabel('Number of features')\nplt.ylabel('Score')\nplt.grid()\nplt.show()\n\n\n\n\nIt seems that when the strength of regularization is low, overfitting is less bad."
  },
  {
    "objectID": "posts/penguins/penguins.html",
    "href": "posts/penguins/penguins.html",
    "title": "Penguins",
    "section": "",
    "text": "import pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\nspecies = [s.split()[0] for s in le.classes_]\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\nWe need to find the combination of features with which we can determine the species of a penguin the fastest. We can accomplish this by running an exhaustive search of all the features contained in this data set. At each iteration, we use the LogisticRegression module to test the score on the current set of features.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", 'Island']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\nbest_score = 0\nbest_col = 0\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    if best_score < LR.score(X_train[cols], y_train):\n        best_col = cols\n        best_score = LR.score(X_train[cols], y_train)\nprint(best_col)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nWe can visualize the training data by using Seaborn and apply it on the most efficient combination of features that we have just found.\n\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_theme()\n\n# Create a visualization\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", col=\"Island\", hue ='Species')\n\n<seaborn.axisgrid.FacetGrid at 0x1f4f9fce4c0>\n\n\n\n\n\nWe can see that the training data is linearly separable. This means that the score on our logistic regression should be 100%.\n\nLR = LogisticRegression()\nLR.fit(X_train[best_col], y_train)\nLR.score(X_train[best_col], y_train)\n\n1.0\n\n\nAnd it is 100%! Next we will try to make some interesting tables from this. First, we will divide the data set by island.\n\ntrain.groupby('Island')[['Culmen Length (mm)', 'Culmen Depth (mm)']].aggregate(min).round(2)\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Island\n      \n      \n    \n  \n  \n    \n      Biscoe\n      34.5\n      13.1\n    \n    \n      Dream\n      32.1\n      15.5\n    \n    \n      Torgersen\n      33.5\n      16.6\n    \n  \n\n\n\n\n\ntrain.groupby('Island')[['Culmen Length (mm)', 'Culmen Depth (mm)']].aggregate(max).round(2)\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Island\n      \n      \n    \n  \n  \n    \n      Biscoe\n      59.6\n      21.1\n    \n    \n      Dream\n      55.8\n      21.2\n    \n    \n      Torgersen\n      46.0\n      21.5\n    \n  \n\n\n\n\nThese tables give the range of culmen length and depth of the penguins on each island. For example, on Biscoe island, the culmen length range is 34.5 mm - 59.6 mm. This means that this island must have Adelie penguins on it because Adelie penguins have the shortest culmens. The 59.6 mm maximum suggests that the other species may be Gentoo or Chinstrap. The tight length range 33.5 mm - 46.0 mm suggests that there are only Adelie penguins on Torgensen island. By making quantitative comparisons like this, we can linearly separate our data.\n\nfrom matplotlib.patches import Patch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[best_col], y_train)\n\n\n\n\nThese 3 graphs correspond to the 3 islands: Biscoe, Dream and Torgensen.\nNow that we have established that “Culmen Length”, “Culmen Depth” and “Island” make the best feature combination, we will test this out on new data.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\nsns.relplot(\n    data=test,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", col=\"Island\", hue ='Species')\n\n<seaborn.axisgrid.FacetGrid at 0x1f4f8b00d00>\n\n\n\n\n\nThe data set, as organized by our feature combination, is again linearly separable. Does this guarantee a score of 100%?\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_col], y_test)\n\n1.0\n\n\nSo it does!"
  },
  {
    "objectID": "posts/perceptron/perceptron.html",
    "href": "posts/perceptron/perceptron.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "The primary method of the Perceptron class is Perceptron.fit(X, y). \\(\\textbf{X} \\in \\mathbb{R}^{n\\times p}\\) is a matrix of predictor variables, with \\(n\\) observations and \\(p\\) features, and \\(\\textbf{y} \\in {0, 1}^n\\) is a vector of binary labels. When the method Perceptron.fit(X, y) is called, the perceptron in question will get an instance variable of weights called w, and a history of the perceptron’s accuracy scores during the runtime of the function, called history.\nBefore updating the perceptron’s weight, we rewrite the problem with \\(\\tilde{\\textbf{X}} = [ \\textbf{X}, \\textbf{1} ]\\) and \\(\\tilde{\\textbf{w}} = (\\textbf{w}, - b)\\) to account for the bias. The perceptron algorithm is implemented as follows:\n\nWe start by assigning a random weight vector \\(\\tilde{\\textbf{w}}^{(0)}\\).\nUntil the perceptron achieves perfect accuracy (or until termination), in each time step t:\n\nPick a random point index \\(i\\) in \\(\\textbf{y}\\)\nUpdate the weight: \\[\\tilde{\\textbf{w}}^{(t+1)} = \\tilde{\\textbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\textbf{w}}^{(t)}, \\tilde{x}_i \\rangle < 0) \\tilde{y}_i \\tilde{x}_i,\\] where \\(\\tilde{y}_i = 2y_i -1\\), which takes on values of -1 and 1 instead of 0 and 1. The Boolean function in this equation checks whether or not the weight needs an update. It does not update when \\(\\tilde{y}_i \\langle \\tilde{\\textbf{w}}^{(t)}, \\tilde{x}_i \\rangle \\geq 0\\), which means that the prediction value matches the real value.\n\n\nWe will use this algorithm to conduct a few experiments, and confirm that if a set of data is linearly separable, this algorithm will separate the data.\n\nExperiment 1\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this experiment, our data have 2 features, and we can see that they are linearly separable. First, we import the Perceptron class from the code that I wrote. After calling the Perceptron.fit(X, y) method, we will find a weight vector \\(\\tilde{\\textbf{w}}\\) that describes the separating line.\n\nfrom perceptron import Perceptron\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nGiven this set of linearly separable data, over time, our perceptron was able to separate the data with 100% accuracy. This was achieved within under 200 time steps.\n\n\nExperiment 2\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (0,0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this experiment, the data overlap each other, which makes them linearly inseparable. If we implement the perceptron algorithm in this case, the method Perceptron.fit(X, y) until it terminates, but its accuracy will never achieve 100%.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nBecause the perceptron algorithm never reached an accuracy of 100%, it was iterated 1000 times until it terminated. This is one of the limitations of the perceptron algorithm.\n\n\nExperiment 3\n\nnp.random.seed(12345)\n\nn = 100\np_features = 8\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = 7)\n\nIn this case, we consider 7 features. Because the data set is no longer 2-dimensional, visualization is not possible. There are 7 centers spread randomly in the \\(\\mathbb{R}^7\\) space, each of which is surrounded by a cluster of one feature.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nThe history of this perceptron indicates that this data set is not linearly separable because the accuracy of the perceptron never reaches 100%, and the algorithm runs for too long.\n\n\nRuntime Complexity\nWe are concerned with the runtime complexity of a single iteration of the perceptron algorithm update. We assume that the relevant operations are addition and multiplication. According to the update function, the operation that takes up the most time is the dot product between \\(\\tilde{\\textbf{w}}\\) and \\(\\tilde{\\textbf{x}}_i\\). Each of these terms has length \\(p+1\\) (\\(p\\) is the number of features). This is a sum of \\(p+1\\) products, which means that this update function has a time complexity of \\(\\mathcal{O}(p)\\). This depends only on the number of features, and not the number of data points."
  },
  {
    "objectID": "posts/unsupervised/unsupervised.html",
    "href": "posts/unsupervised/unsupervised.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "Experiment\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\n\nurl = \"https://pbs.twimg.com/media/E-z1YOxUcAAy5HI?format=jpg&name=4096x4096\"\n\nimg = read_image(url)\n\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\nAbove is my favorite character from my favorite game, whose image has been grayscaled.\nAn \\(m \\times n\\) greyscale image needs \\(mn\\) numbers to represent it. If we use the SVD to compress the image using k components, we need to store the following matrices:\n\nThe \\(m \\times k\\) matrix \\(\\textbf{U}\\)\nThe \\(k \\times k\\) matrix \\(\\textbf{D}\\)\nThe \\(k \\times n\\) matrix \\(\\textbf{V}\\)\n\nThe total numbers needed to reconstruct the image is then \\(mk+k^2+kn\\). Using this, we get the formula for the amount of storage needed for a reconstruction as a fraction of the amount of storage needed for the original image:\n\\[ \\frac{mk+k^2+kn}{mn} \\times 100 \\%\\]\n\ndef svd_reconstruct(image, k):\n    # Compute the singular value decomposition of the image\n    U, sigma, V = np.linalg.svd(image, full_matrices=False)\n\n    # Truncate the matrices to keep only the first k singular values\n    Uk = U[:, :k]\n    Vk = V[:k, :]\n    \n    D = np.zeros_like(image,dtype=float) # matrix of zeros of same shape as A\n    D[:min(image.shape),:min(image.shape)] = np.diag(sigma) \n    Dk = D[:k, :k]\n    \n    # Reconstruct the image using the truncated matrices\n    reconstructed_image = Uk@Dk@Vk\n    return reconstructed_image\n\n\ndef svd_experiment(image, k_list):\n    # Presents a series of reconstructions of the same image for different values of k\n  \n    for k in k_list: # k_list is a list or array of k values\n        fig, axarr = plt.subplots()\n        hello = svd_reconstruct(image, k) # reconstruct image\n        m, n = image.shape\n        mn = m*n\n        stored = m*k + k**2 + k*n\n        axarr.imshow(hello, cmap = \"Greys\")\n        axarr.axis(\"off\")\n        axarr.set(title = str(k) + 'components, % storage = ' + str(round(stored/mn*100, 3)))\n\n    plt.show()\n\n\nhi = np.arange(5,81,5)\nsvd_experiment(grey_img, hi)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncompare_images(grey_img, svd_reconstruct(grey_img, 80))\n\n\n\n\nUsing only 6.5% of the initial storage for the original image, we can still make out what it is about: Razor eating some meat, out in the wild.\n\n\nPart 2: Spectral Community Detection\nWe will move on to identifying clusters in point cloud data sets. Below is a function that uses the Laplacian Spectral Clustering method to assign binary labels to nodes of a social network.\n\nimport networkx as nx\n\ndef spectral_clustering(G):\n    # Get the adjacency matrix of the graph\n    A = nx.adjacency_matrix(G).toarray()\n\n    # Compute the degree matrix of the graph\n    D = np.diag(np.sum(A, axis=1))\n\n    # Compute the Laplacian matrix of the graph\n    L = np.linalg.inv(D)@(D - A)\n\n    # Compute the eigenvectors of the Laplacian matrix\n    eigvals, eigvecs = np.linalg.eig(L)\n\n    # Sort the eigenvectors by their corresponding eigenvalues\n    sorted_indices = np.argsort(eigvals)\n    sorted_eigvecs = eigvecs[:, sorted_indices]\n\n    # Cluster the nodes based on the second smallest eigenvector\n    labels = sorted_eigvecs[:, 1] > 0\n\n    return labels*1\n\nWe will use the karate club social network.\n\nG = nx.karate_club_graph()\nlayout = nx.layout.fruchterman_reingold_layout(G)\nnx.draw(G, layout, with_labels=True, node_color = \"steelblue\")\n\n\n\n\nOur code predicts where each member would end up if the club got divided into 2. We will compare our code’s prediction to what actually happened.\n\nclubs = spectral_clustering(G)\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == 0 else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\") \n\nC:\\Users\\ledtr\\AppData\\Local\\Temp\\ipykernel_20396\\3045702461.py:5: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n  A = nx.adjacency_matrix(G).toarray()\n\n\n\n\n\nAnd below is the actual division that happened.\n\nclubs = nx.get_node_attributes(G, \"club\")\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == \"Officer\" else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        ) \n\n\n\n\nOur code came pretty close to reality. The only difference is that while our code predicted that the eight member would end up in Officer’s club, they actually were in Mr. Hi’s club. A lot of factors can contribute to 8’s decision to stick with Mr. Hi: parental pressure (they may have said something like you have to stick by your mentor etc.), social hierarchy (8 was afraid of upsetting Mr. Hi), etc. With all these factors, I’m surprised that this predictive model was almost accurate. In order to say whether other social phenomena can be predicted this way, we need other social networks with binary outcomes such as this example. I tried digging on the Internet but every post about spectral clustering only mentions this example."
  }
]