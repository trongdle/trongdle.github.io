[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am Trong, a recent graduate from Middlebury College with a major in Physics and minor in Computer Science. This blog outlines my journey towards a basic understanding of machine learning and deep learning, and my plan to incorporate this knowledge into physics studies."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introductory Machine Learning",
    "section": "",
    "text": "We used deep learning to guess a patient’s race based on their chest X-ray.\n\n\n\n\n\n\nMay 10, 2023\n\n\nTrong Le, Jay-U Chung, Kent Canonigo\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nThe Adam optimizer and some simple experiments.\n\n\n\n\n\n\nApr 17, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nLinear algebra methods for unsupervised learning with two kinds of data: images and graphs\n\n\n\n\n\n\nApr 13, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog post, I will create a machine learning model that predicts an individual characteristic like employment status or income on the basis of other demographic characteristics.\n\n\n\n\n\n\nMar 25, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog post, I will implement least-squares linear regression, and experiment with LASSO regularization for overparameterized problems.\n\n\n\n\n\n\nMar 23, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nClassifying Palmer Penguins with Sklearn.\n\n\n\n\n\n\nMar 7, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nOptimization algorithms based on the gradients of functions.\n\n\n\n\n\n\nFeb 26, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog post, I will implement the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets.\n\n\n\n\n\n\nFeb 19, 2023\n\n\nTrong Le\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/adam/adam.html",
    "href": "posts/adam/adam.html",
    "title": "Optimization with Adam",
    "section": "",
    "text": "A. Stochastic Gradient Descent vs. Adam Optimization\nWe will use the data given below to compare between stochastic logistic regression and Adam optimization.\n\nfrom solutions import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .01)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit_adam(X, y, batch_size=10, alpha=0.001, beta1=0.9, beta2=0.999, w_0 = None, epsilon=1e-8, m_epochs=10000)\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Adam gradient\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Empirical Risk\")\nlegend = plt.legend() \n\n\n\n\nThe learning rate for the stochastic gradient method is 10 times bigger than that for the Adam method, but the Adam method converges in half the time steps!\n\n\nB. Digits Experiment\nWe will use the digits dataset from sklearn. I will filter the digits data set so that it only contains data with two class labels, namely 0s and 1s.\n\nfrom sklearn.datasets import load_digits\nfrom sklearn.model_selection import train_test_split\n\n# Load the dataset\ndigits = load_digits()\n\n# Filter the data to only contain 4s and 8s\nX = digits.data[(digits.target == 0) | (digits.target == 1)]\ny = digits.target[(digits.target == 0) | (digits.target == 1)]\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n\nLR_momentum = LogisticRegression()\n\nLR_momentum.fit_stochastic(X_train/255, y_train, \n            m_epochs = 10000, \n            momentum = True, \n            batch_size = 10, \n            alpha = .1)\nnum_steps = len(LR_momentum.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_momentum.loss_history, label = \"accelerated stochastic gradient\")\n\nLR_Adam = LogisticRegression()\nLR_Adam.fit_adam(X_train/255, y_train, batch_size=10, alpha=0.001, beta1=0.9, beta2=0.999, w_0 = None, epsilon=1e-8, m_epochs=10000)\nnum_steps = len(LR_Adam.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR_Adam.loss_history, label = \"Adam\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Empirical Risk\")\nlegend = plt.legend() \n\n\n\n\nSo it seems that the Adam optimization is much faster than the stochastic gradient with momentum method! And now we will compare their efficiencies on unseen data.\n\nprint('The score of the Adam machine is ' + str(LR_Adam.score(X_test/255, y_test)) + '.')\nprint('The score of the accelerated stochastic gradient method is ' + str(LR_momentum.score(X_test/255, y_test)) + '.')\nprint('The loss of the Adam machine is ' + str(LR_Adam.loss(X_test/255, y_test)) + '.')\nprint('The loss of the accelerated stochastic gradient method is ' + str(LR_momentum.loss(X_test/255, y_test)) + '.')\n\nThe score of the Adam machine is 1.0.\nThe score of the accelerated stochastic gradient method is 1.0.\nThe loss of the Adam machine is 0.49792824663572666.\nThe loss of the accelerated stochastic gradient method is 0.49792824663572666.\n\n\nThese numbers look the same, but the Adam machine achieved these numbers in a much faster fashion.\n\n\nOne More Experiment\nI will do one more experiment on a dataset that looks like the one in section A, except that there are 80 features.\n\n# make the data\np_features = 81\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .01)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit_adam(X, y, batch_size=10, alpha=0.001, beta1=0.9, beta2=0.999, w_0 = None, epsilon=1e-8, m_epochs=10000)\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"Adam gradient\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Empirical Risk\")\nlegend = plt.legend() \n\n\n\n\nThe Adam algorithm again comes out on top!"
  },
  {
    "objectID": "posts/audit/audit.html",
    "href": "posts/audit/audit.html",
    "title": "Auditing Allocative Bias",
    "section": "",
    "text": "from folktables import ACSDataSource, ACSEmployment, BasicProblem, adult_filter\nimport numpy as np\n\nSTATE = \"TX\"\n\ndata_source = ACSDataSource(survey_year='2018', \n                            horizon='1-Year', \n                            survey='person')\n\nacs_data = data_source.get_data(states=[STATE], download=True)\n\nacs_data.head()\n\n\n\n\n\n  \n    \n      \n      RT\n      SERIALNO\n      DIVISION\n      SPORDER\n      PUMA\n      REGION\n      ST\n      ADJINC\n      PWGTP\n      AGEP\n      ...\n      PWGTP71\n      PWGTP72\n      PWGTP73\n      PWGTP74\n      PWGTP75\n      PWGTP76\n      PWGTP77\n      PWGTP78\n      PWGTP79\n      PWGTP80\n    \n  \n  \n    \n      0\n      P\n      2018GQ0000026\n      7\n      1\n      5000\n      3\n      48\n      1013097\n      29\n      21\n      ...\n      29\n      4\n      5\n      54\n      27\n      52\n      28\n      29\n      52\n      29\n    \n    \n      1\n      P\n      2018GQ0000057\n      7\n      1\n      6601\n      3\n      48\n      1013097\n      16\n      19\n      ...\n      33\n      3\n      18\n      16\n      32\n      3\n      18\n      16\n      2\n      2\n    \n    \n      2\n      P\n      2018GQ0000070\n      7\n      1\n      4302\n      3\n      48\n      1013097\n      64\n      24\n      ...\n      14\n      64\n      110\n      62\n      14\n      15\n      64\n      64\n      13\n      67\n    \n    \n      3\n      P\n      2018GQ0000079\n      7\n      1\n      700\n      3\n      48\n      1013097\n      260\n      20\n      ...\n      57\n      451\n      261\n      272\n      59\n      477\n      261\n      258\n      480\n      56\n    \n    \n      4\n      P\n      2018GQ0000082\n      7\n      1\n      900\n      3\n      48\n      1013097\n      12\n      31\n      ...\n      10\n      3\n      11\n      22\n      12\n      20\n      1\n      12\n      21\n      1\n    \n  \n\n5 rows × 286 columns\n\n\n\n\npossible_features=['AGEP', 'SCHL', 'MAR', 'RELP', 'DIS', 'ESP', 'CIT', 'MIG', 'MIL', 'ANC', 'NATIVITY', 'DEAR', 'DEYE', 'DREM', 'SEX', 'RAC1P', 'ESR']\nacs_data[possible_features].head()\n\n\n\n\n\n  \n    \n      \n      AGEP\n      SCHL\n      MAR\n      RELP\n      DIS\n      ESP\n      CIT\n      MIG\n      MIL\n      ANC\n      NATIVITY\n      DEAR\n      DEYE\n      DREM\n      SEX\n      RAC1P\n      ESR\n    \n  \n  \n    \n      0\n      21\n      16.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      2\n      1.0\n    \n    \n      1\n      19\n      16.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      6.0\n    \n    \n      2\n      24\n      12.0\n      5\n      16\n      1\n      NaN\n      1\n      1.0\n      4.0\n      1\n      1\n      2\n      2\n      1.0\n      1\n      2\n      6.0\n    \n    \n      3\n      20\n      16.0\n      5\n      17\n      2\n      NaN\n      1\n      1.0\n      3.0\n      1\n      1\n      2\n      2\n      2.0\n      2\n      1\n      1.0\n    \n    \n      4\n      31\n      17.0\n      5\n      17\n      2\n      NaN\n      1\n      3.0\n      4.0\n      4\n      1\n      2\n      2\n      2.0\n      1\n      1\n      1.0\n    \n  \n\n\n\n\n\nfeatures_to_use = [f for f in possible_features if f not in [\"ESR\", \"RAC1P\"]]\n\n\nEmploymentProblem = BasicProblem(\n    features=features_to_use,\n    target='ESR',\n    target_transform=lambda x: x == 1,\n    group='RAC1P',\n    preprocess=lambda x: x,\n    postprocess=lambda x: np.nan_to_num(x, -1),\n)\n\nfeatures, label, group = EmploymentProblem.df_to_numpy(acs_data)\n\n\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test, group_train, group_test = train_test_split(\n    features, label, group, test_size=0.2, random_state=0)\n\n\nimport pandas as pd\ndf = pd.DataFrame(X_train, columns = features_to_use)\ndf[\"group\"] = group_train\ndf[\"label\"] = y_train\n\n\nprint('1. There are ' + str(len(df)) + ' individuals in this survey.')\nprint('2. Of those individuals, ' + str(len(df[df['label'] == 1])) + ' are employed.')\n\n1. There are 214480 individuals in this survey.\n2. Of those individuals, 96805 are employed.\n\n\n\nprint('3. The number of individuals in each racial group is: \\n' + str(df.groupby('group')['label'].aggregate(len)))\n\n3. The number of individuals in each racial group is: \ngroup\n1    165969\n2     20614\n3       836\n4        14\n5       401\n6     10141\n7       158\n8     10544\n9      5803\nName: label, dtype: int64\n\n\n\nprint('4. The proportion of employed individuals in each racial group is: \\n' + str(df[df['label'] == 1].groupby('group')['label'].aggregate(len)/df.groupby('group')['label'].aggregate(len)))\n\n4. The proportion of employed individuals in each racial group is: \ngroup\n1    0.455242\n2    0.416756\n3    0.431818\n4    0.357143\n5    0.461347\n6    0.499359\n7    0.430380\n8    0.466711\n9    0.353955\nName: label, dtype: float64\n\n\n\nprint('5.1. The proportion of employed male individuals in each racial group is: \\n' + str(df[df['SEX'] == 1][df['label'] == 1].groupby('group')['label'].aggregate(len)/df[df['SEX'] == 1].groupby('group')['label'].aggregate(len)))\n\n5.1. The proportion of employed male individuals in each racial group is: \ngroup\n1    0.502327\n2    0.396673\n3    0.474178\n4    0.250000\n5    0.488263\n6    0.553741\n7    0.428571\n8    0.529160\n9    0.371400\nName: label, dtype: float64\n\n\nC:\\Users\\ledtr\\AppData\\Local\\Temp\\ipykernel_5008\\3156142459.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  print('5.1. The proportion of employed male individuals in each racial group is: \\n' + str(df[df['SEX'] == 1][df['label'] == 1].groupby('group')['label'].aggregate(len)/df[df['SEX'] == 1].groupby('group')['label'].aggregate(len)))\n\n\n\nprint('5.2. The proportion of employed female individuals in each racial group is: \\n' + str(df[df['SEX'] == 2][df['label'] == 1].groupby('group')['label'].aggregate(len)/df[df['SEX'] == 2].groupby('group')['label'].aggregate(len)))\n\n5.2. The proportion of employed female individuals in each racial group is: \ngroup\n1    0.410106\n2    0.435596\n3    0.387805\n4    0.500000\n5    0.430851\n6    0.447318\n7    0.432836\n8    0.401550\n9    0.335905\nName: label, dtype: float64\n\n\nC:\\Users\\ledtr\\AppData\\Local\\Temp\\ipykernel_5008\\266627361.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n  print('5.2. The proportion of employed female individuals in each racial group is: \\n' + str(df[df['SEX'] == 2][df['label'] == 1].groupby('group')['label'].aggregate(len)/df[df['SEX'] == 2].groupby('group')['label'].aggregate(len)))\n\n\nExcept for groups 2, 4 and 7, the other racial groups have a higher proportion of employed men than women.\nNow we will train our model. I’m using the DecisionTreeClassifier model. Below is my code to see which maximum depth of the tree gives the most accuracy. The cross-validation is only done on the training data.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\n\ncross_train, cross_test, crossy_train, crossy_test = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=0)\nscore_i = []\n\nfor i in range(1, 30):\n    model = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth = i))\n    model.fit(cross_train, crossy_train)\n    y_hat = model.predict(cross_test)\n    score = (y_hat == crossy_test).mean()\n    score_i.append(score)\n\nplt.plot(np.arange(1, 30), score_i)\nplt.xlabel('max_depth')\nplt.ylabel('score')\nplt.grid()\n\n\n\n\nIt seems that max_depth = 9 is our optimal tree depth. We will now test this model on test data.\n\nmodel = make_pipeline(StandardScaler(), DecisionTreeClassifier(max_depth = 9))\nmodel.fit(X_train, y_train)\ny_hat = model.predict(X_test)\nscore = (y_hat == y_test).mean()\nprint('1. The overall accuracy of the model is ' + str(score.round(4)) + '.')\n\nfrom sklearn.metrics import confusion_matrix\nC = confusion_matrix(y_test, y_hat, normalize='true')\nTN = C[0][0]\nFP = C[0][1]\nFN = C[1][0]\nTP = C[1][1]\nPPV = TP/(TP+FP)\nprint('2. The overall positive predictive value is ' + str(PPV.round(4)) + '.')\nprint('3. The overall false negative rate is ' + str(FN.round(4)) + ' and false positive rate is ' + str(FP.round(4)) + '.')\n\n1. The overall accuracy of the model is 0.8237.\n2. The overall positive predictive value is 0.8045.\n3. The overall false negative rate is 0.1361 and false positive rate is 0.2099.\n\n\n\nsub_acc = []\nsub_PPV = []\nsub_FNR = []\nsub_FPR = []\n\nfor i in range(1,10):\n    y_hat = model.predict(X_test)\n    score = (y_hat[group_test == i] == y_test[group_test == i]).mean()\n    sub_acc.append(score)\n    C = confusion_matrix(y_test[group_test == i], y_hat[group_test == i], normalize='true')\n    TN = C[0][0]\n    FP = C[0][1]\n    FN = C[1][0]\n    TP = C[1][1]\n    PPV = TP/(TP+FP)\n    sub_PPV.append(PPV)\n    sub_FNR.append(FN)\n    sub_FPR.append(FP)\n\nNow we will look at these numbers in each subgroup. The information is displayed in the following graphs.\n\nfig1, ax1 = plt.subplots()\nhello = np.arange(1,10)\nax1.plot(hello, sub_acc)\nax1.set_xlabel('group')\nax1.set_ylabel('score')\nax1.set_title('Subgroup accuracy')\nax1.grid()\n\nfig2, ax2 = plt.subplots()\nax2.plot(hello, sub_PPV)\nax2.set_xlabel('group')\nax2.set_ylabel('Positive predictive value')\nax2.set_title('Subgroup PPV')\nax2.grid()\n\nfig3, ax3 = plt.subplots()\nax3.plot(hello, sub_FNR, label = 'False Negative Rate')\nax3.plot(hello, sub_FPR, label = 'False Positive Rate')\nax3.set_xlabel('group')\nax3.set_ylabel('False rate')\nax3.set_title('Subgroup False Prediction Rate')\nax3.legend()\nax3.grid()\n\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThis model is approximately calibrated. If we exclude group 4, which has a small sample size of 14 individuals, the positive predictive value for each group is close to the overall PPV. It is the same case for the FNR and FPR, which means that this model also satisfies approximate error rate balance. This means that our model has the same prevalence for all groups, which means that this is an amazing model!\n\nConclusion\n\nThis model can be used by job referrals websites such as LinkedIn to advertise to potentially unemployed people. It can also be used by car insurance companies to advertise to potentially employed people, who may own a car to commute to work. Or it can be used by apartment complexes to advertise to employed people.\nI think that if this model is used in a governmental setting, the model can predict the employability of a person based on their features excluding race. This can be used to decide whether a person is eligible for a loan (if they can pay back the loan by working at a job), whether they can rent a place in a certain area (expensive areas require a stable and supple income).\nBased on my audit, my model does not have any problematic bias.\nA problem with my model is that it cannot accurately predict for racial groups that have too few individuals."
  },
  {
    "objectID": "posts/gradient/gradient.html",
    "href": "posts/gradient/gradient.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "Below is our data, which is not linearly separable.\n\nfrom solutions import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\ndef sigmoid(z):\n    # the sigmoid function\n    return 1 / (1 + np.exp(-z))\n\n\nn, p = X.shape\n\n# weight tilda and X tilda that has one extra term\nw_tilda = np.random.rand(p+1)\nX_tilda = np.append(X, np.ones((X.shape[0], 1)), 1)\n\ny_hat = np.dot(X_tilda, w_tilda) #inner product between X and w\ndeltaL = 1/n*(sigmoid(y_hat) - y)@X_tilda\ndeltaL\n\narray([-0.22233893, -0.1366533 ,  0.09319262])\n\n\n\n1. Gradient Descent\nThe gradient descent algorithm is implemented when the LogisticRegression.fit(X, y) is called. \\(\\textbf{X}\\) is the feature matrix, and \\(\\textbf{y}\\) is the target vector, as we know from the perceptron blog post. We need to find a weight vector that can minimize the logistic loss on \\(\\textbf{X}\\) and \\(\\textbf{y}\\) by updating the weight in a loop. At every loop:\n\ncompute the gradient descent of the logistic loss, which is given by \\[\\nabla L(\\textbf{w}) = \\frac{1}{n} \\sum^n_{i=1} (\\sigma (\\langle \\textbf{w}, \\textbf{x}_i \\rangle), y_i) \\textbf{w}_i, \\] where \\(\\sigma(z)\\) denotes the sigmoid function.\nupdate the weight until the logistic loss does not change \\[\\textbf{w}^{(t+1)} = \\textbf{w}^{(t)} - \\alpha \\nabla L(\\textbf{w}^{(t)})\\]\n\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 10, max_epochs = 1000)\n\n# inspect the fitted value of w\nLR.w \n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfigy, axy = plt.subplots()\naxy.scatter(X[:,0], X[:,1], c = y)\ndraw_line(LR.w, -2, 2)\naxy.set_xlabel(\"Feature 1\")\naxy.set_ylabel(\"Feature 2\")\n\nfigx, axv = plt.subplots()\naxv.plot(LR.loss_history)\naxv.set_xlabel(\"Iteration\")\naxv.set_ylabel(\"Empirical Risk\")\nplt.show()\n\n\n\n\n\n\n\nIn this case, the learning rate was set very high to 10, which caused the algorithm to converge too soon and leave the logistic loss at 0.24. As we change the learning rate to 0.1, we can see that the loss fell to 0.24.\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha=0.1, max_epochs=1000)\n\nfigy, axy = plt.subplots()\naxy.scatter(X[:,0], X[:,1], c = y)\ndraw_line(LR.w, -2, 2)\naxy.set_xlabel(\"Feature 1\")\naxy.set_ylabel(\"Feature 2\")\n\nfigx, axv = plt.subplots()\naxv.plot(LR.loss_history)\naxv.set_xlabel(\"Iteration\")\naxv.set_ylabel(\"Empirical Risk\")\nplt.show()\n\n\n\n\n\n\n\n\n\n2. Stochastic Gradient Descent\nThe stochastic gradient descent algorithm is implemented by the LogisticRegression.fit_stochastic(X, y) method. In this algorithm, instead of computing the complete gradient as in part 1, we compute a stochastic gradient by picking a random subset \\(S \\subseteq [n] = \\{1, ..., n\\}\\) and computing\n\\[\\nabla_S L(\\textbf{w}) = \\frac{1}{|S|} \\sum_{i \\in S} \\nabla (\\sigma (\\langle \\textbf{w}, \\textbf{x}_i \\rangle), y_i)\\textbf{x}_i \\]\nThe size of \\(S\\) is the batch size, which we can refer to as \\(k\\). The batch gradient descent is performed as follows: 1. Shuffle the points randomly. 2. Pick the first \\(k\\) random points, compute the stochastic gradient, and then perform an update. 3. Pick the next \\(k\\) random points and repeat.. 4. When we have gone through all \\(n\\) points, reshuffle them all randomly and proceed again.\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this experiment, we can see how the stochastic gradient descent algorithm can outperform the original gradient descent method.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 1000, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .1, max_epochs = 1000)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Empirical Risk\")\nlegend = plt.legend() \n\n\n\n\nThe stochastic gradient method used only 150 loops, while the gradient method still had not converged at loop 1000.\n\n\n3. Stochastic Gradient Descent with Momentum\nThis only difference in this method comes from the momentum factor \\(\\beta\\). The weight update is given by\n\\[\\textbf{w}^{(t+1)} = \\textbf{w}^{(t)} - \\alpha \\nabla L(\\textbf{w}^{(t)}) + \\beta (\\textbf{w}^{(t)}-\\textbf{w}^{(t-1)})\\]\nThe idea here is that if the previous weight update was good, we may want to continue moving along this direction. To test how the momentum can help with accelerating convergence, we choose a small sample size of 150, distributed among 10 features.\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 11\nX, y = make_blobs(n_samples = 150, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\nlegend = plt.legend() \nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Empirical Risk\")\nplt.show()\n\n\n\n\nThe momentum algorithm converges much faster than the other method."
  },
  {
    "objectID": "posts/linear-regression/linear-regression.html",
    "href": "posts/linear-regression/linear-regression.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "The optimization for linear regression can be implemented by using my source code linear.py, which can be found HERE. In this code, loss optimization is implemented in two ways: using gradient descent, and using the optimal weight formula. We will check if this code is implemented correctly.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\nfrom linear import LinearRegression\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.6311\nValidation score = 0.6072\n\n\n\nLR.w.reshape(-1,1)\n\narray([[1.05653571],\n       [0.96877292]])\n\n\n\nLR2 = LinearRegression()\ny2 = y_train.reshape(-1,1)\nLR2.fit_gradient(X_train, y2, alpha=0.01)\nLR2.w\n\narray([[1.10993122],\n       [0.94029021]])\n\n\nSo the weights in both cases are very similar, which means that the gradient method is behaving as it should.\nThe score also increases monotonically in each iteration.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")"
  },
  {
    "objectID": "posts/linear-regression/linear-regression.html#experiments",
    "href": "posts/linear-regression/linear-regression.html#experiments",
    "title": "Implementing Linear Regression",
    "section": "Experiments",
    "text": "Experiments\nNow that we have established that our LinearRegression module is behaving okay, we can move on to experiments, where we increase the number of features and observe the change in validation scores.\n\nn_train = 100\nn_val = 100\np_features = np.arange(1,100)\nnoise = 0.2\ntrain_score = []\ntest_score = []\n\n# create some data\nfor i in range(len(p_features)):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features[i], noise)\n    LR = LinearRegression()\n    LR.fit_analytic(X_train, y_train)\n    train_score.append(LR.score(X_train, y_train).round(4))\n    test_score.append(LR.score(X_val, y_val).round(4))\n\nplt.plot(p_features, train_score, label = 'Training score')\nplt.plot(p_features, test_score, label = 'Validation score')\nplt.legend()\nplt.xlabel('Number of features')\nplt.ylabel('Score')\nplt.grid()\nplt.show()\n\n\n\n\nAs the number of features increases, the training score approaches 100% while validation score becomes noisier. This demonstrates the effect of overfitting: because the machine is trained to account for every little detail of the training dataset, it becomes too inflexible to work with unseen data."
  },
  {
    "objectID": "posts/linear-regression/linear-regression.html#lasso-regularization",
    "href": "posts/linear-regression/linear-regression.html#lasso-regularization",
    "title": "Implementing Linear Regression",
    "section": "LASSO Regularization",
    "text": "LASSO Regularization\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\n\nn_train = 100\nn_val = 100\np_features = np.arange(1,100)\nnoise = 0.2\ntrain_score = []\ntest_score = []\n\n# create some data\nfor i in range(len(p_features)):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features[i], noise)\n    L = Lasso(alpha = 0.001)\n    L.fit(X_train, y_train)\n    train_score.append(L.score(X_train, y_train).round(4))\n    test_score.append(L.score(X_val, y_val).round(4))\n\nplt.plot(p_features, train_score, label = 'Training score')\nplt.plot(p_features, test_score, label = 'Validation score')\nplt.legend()\nplt.xlabel('Number of features')\nplt.ylabel('Score')\nplt.grid()\nplt.show()\n\n\n\n\nIt seems that even when the number of features used is large, the validation score isn’t affected as badly as in the case of pure linear regression. Now we change the strength of regularization to see what changes.\n\ntrain_score = []\ntest_score = []\n\n# create some data\nfor i in range(len(p_features)):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features[i], noise)\n    L = Lasso(alpha = 0.01)\n    L.fit(X_train, y_train)\n    train_score.append(L.score(X_train, y_train).round(4))\n    test_score.append(L.score(X_val, y_val).round(4))\n\nplt.plot(p_features, train_score, label = 'Training score')\nplt.plot(p_features, test_score, label = 'Validation score')\nplt.legend()\nplt.xlabel('Number of features')\nplt.ylabel('Score')\nplt.grid()\nplt.show()\n\n\n\n\n\ntrain_score = []\ntest_score = []\n\n# create some data\nfor i in range(len(p_features)):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features[i], noise)\n    L = Lasso(alpha = 0.0005)\n    L.fit(X_train, y_train)\n    train_score.append(L.score(X_train, y_train).round(4))\n    test_score.append(L.score(X_val, y_val).round(4))\n\nplt.plot(p_features, train_score, label = 'Training score')\nplt.plot(p_features, test_score, label = 'Validation score')\nplt.legend()\nplt.xlabel('Number of features')\nplt.ylabel('Score')\nplt.grid()\nplt.show()\n\n\n\n\nIt seems that when the strength of regularization is low, overfitting is less bad."
  },
  {
    "objectID": "posts/penguins/penguins.html",
    "href": "posts/penguins/penguins.html",
    "title": "Penguins",
    "section": "",
    "text": "import pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\nspecies = [s.split()[0] for s in le.classes_]\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\nWe need to find the combination of features with which we can determine the species of a penguin the fastest. We can accomplish this by running an exhaustive search of all the features contained in this data set. At each iteration, we use the LogisticRegression module to test the score on the current set of features.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", 'Island']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\nbest_score = 0\nbest_col = 0\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    if best_score < LR.score(X_train[cols], y_train):\n        best_col = cols\n        best_score = LR.score(X_train[cols], y_train)\nprint(best_col)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nWe can visualize the training data by using Seaborn and apply it on the most efficient combination of features that we have just found.\n\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_theme()\n\n# Create a visualization\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", col=\"Island\", hue ='Species')\n\n<seaborn.axisgrid.FacetGrid at 0x1f4f9fce4c0>\n\n\n\n\n\nWe can see that the training data is linearly separable. This means that the score on our logistic regression should be 100%.\n\nLR = LogisticRegression()\nLR.fit(X_train[best_col], y_train)\nLR.score(X_train[best_col], y_train)\n\n1.0\n\n\nAnd it is 100%! Next we will try to make some interesting tables from this. First, we will divide the data set by island.\n\ntrain.groupby('Island')[['Culmen Length (mm)', 'Culmen Depth (mm)']].aggregate(min).round(2)\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Island\n      \n      \n    \n  \n  \n    \n      Biscoe\n      34.5\n      13.1\n    \n    \n      Dream\n      32.1\n      15.5\n    \n    \n      Torgersen\n      33.5\n      16.6\n    \n  \n\n\n\n\n\ntrain.groupby('Island')[['Culmen Length (mm)', 'Culmen Depth (mm)']].aggregate(max).round(2)\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Island\n      \n      \n    \n  \n  \n    \n      Biscoe\n      59.6\n      21.1\n    \n    \n      Dream\n      55.8\n      21.2\n    \n    \n      Torgersen\n      46.0\n      21.5\n    \n  \n\n\n\n\nThese tables give the range of culmen length and depth of the penguins on each island. For example, on Biscoe island, the culmen length range is 34.5 mm - 59.6 mm. This means that this island must have Adelie penguins on it because Adelie penguins have the shortest culmens. The 59.6 mm maximum suggests that the other species may be Gentoo or Chinstrap. The tight length range 33.5 mm - 46.0 mm suggests that there are only Adelie penguins on Torgensen island. By making quantitative comparisons like this, we can linearly separate our data.\n\nfrom matplotlib.patches import Patch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[best_col], y_train)\n\n\n\n\nThese 3 graphs correspond to the 3 islands: Biscoe, Dream and Torgensen.\nNow that we have established that “Culmen Length”, “Culmen Depth” and “Island” make the best feature combination, we will test this out on new data.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\nsns.relplot(\n    data=test,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", col=\"Island\", hue ='Species')\n\n<seaborn.axisgrid.FacetGrid at 0x1f4f8b00d00>\n\n\n\n\n\nThe data set, as organized by our feature combination, is again linearly separable. Does this guarantee a score of 100%?\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_col], y_test)\n\n1.0\n\n\nSo it does!"
  },
  {
    "objectID": "posts/perceptron/perceptron.html",
    "href": "posts/perceptron/perceptron.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "The primary method of the Perceptron class is Perceptron.fit(X, y). \\(\\textbf{X} \\in \\mathbb{R}^{n\\times p}\\) is a matrix of predictor variables, with \\(n\\) observations and \\(p\\) features, and \\(\\textbf{y} \\in {0, 1}^n\\) is a vector of binary labels. When the method Perceptron.fit(X, y) is called, the perceptron in question will get an instance variable of weights called w, and a history of the perceptron’s accuracy scores during the runtime of the function, called history.\nBefore updating the perceptron’s weight, we rewrite the problem with \\(\\tilde{\\textbf{X}} = [ \\textbf{X}, \\textbf{1} ]\\) and \\(\\tilde{\\textbf{w}} = (\\textbf{w}, - b)\\) to account for the bias. The perceptron algorithm is implemented as follows:\n\nWe start by assigning a random weight vector \\(\\tilde{\\textbf{w}}^{(0)}\\).\nUntil the perceptron achieves perfect accuracy (or until termination), in each time step t:\n\nPick a random point index \\(i\\) in \\(\\textbf{y}\\)\nUpdate the weight: \\[\\tilde{\\textbf{w}}^{(t+1)} = \\tilde{\\textbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\textbf{w}}^{(t)}, \\tilde{x}_i \\rangle < 0) \\tilde{y}_i \\tilde{x}_i,\\] where \\(\\tilde{y}_i = 2y_i -1\\), which takes on values of -1 and 1 instead of 0 and 1. The Boolean function in this equation checks whether or not the weight needs an update. It does not update when \\(\\tilde{y}_i \\langle \\tilde{\\textbf{w}}^{(t)}, \\tilde{x}_i \\rangle \\geq 0\\), which means that the prediction value matches the real value.\n\n\nWe will use this algorithm to conduct a few experiments, and confirm that if a set of data is linearly separable, this algorithm will separate the data.\n\nExperiment 1\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this experiment, our data have 2 features, and we can see that they are linearly separable. First, we import the Perceptron class from the code that I wrote. After calling the Perceptron.fit(X, y) method, we will find a weight vector \\(\\tilde{\\textbf{w}}\\) that describes the separating line.\n\nfrom perceptron import Perceptron\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nGiven this set of linearly separable data, over time, our perceptron was able to separate the data with 100% accuracy. This was achieved within under 200 time steps.\n\n\nExperiment 2\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (0,0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this experiment, the data overlap each other, which makes them linearly inseparable. If we implement the perceptron algorithm in this case, the method Perceptron.fit(X, y) until it terminates, but its accuracy will never achieve 100%.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nBecause the perceptron algorithm never reached an accuracy of 100%, it was iterated 1000 times until it terminated. This is one of the limitations of the perceptron algorithm.\n\n\nExperiment 3\n\nnp.random.seed(12345)\n\nn = 100\np_features = 8\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = 7)\n\nIn this case, we consider 7 features. Because the data set is no longer 2-dimensional, visualization is not possible. There are 7 centers spread randomly in the \\(\\mathbb{R}^7\\) space, each of which is surrounded by a cluster of one feature.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nThe history of this perceptron indicates that this data set is not linearly separable because the accuracy of the perceptron never reaches 100%, and the algorithm runs for too long.\n\n\nRuntime Complexity\nWe are concerned with the runtime complexity of a single iteration of the perceptron algorithm update. We assume that the relevant operations are addition and multiplication. According to the update function, the operation that takes up the most time is the dot product between \\(\\tilde{\\textbf{w}}\\) and \\(\\tilde{\\textbf{x}}_i\\). Each of these terms has length \\(p+1\\) (\\(p\\) is the number of features). This is a sum of \\(p+1\\) products, which means that this update function has a time complexity of \\(\\mathcal{O}(p)\\). This depends only on the number of features, and not the number of data points."
  },
  {
    "objectID": "posts/unsupervised/unsupervised.html",
    "href": "posts/unsupervised/unsupervised.html",
    "title": "Unsupervised Learning with Linear Algebra",
    "section": "",
    "text": "Experiment\n\nimport PIL\nimport urllib\n\ndef read_image(url):\n    return np.array(PIL.Image.open(urllib.request.urlopen(url)))\n\n\nurl = \"https://pbs.twimg.com/media/E-z1YOxUcAAy5HI?format=jpg&name=4096x4096\"\n\nimg = read_image(url)\n\n\nfig, axarr = plt.subplots(1, 2, figsize = (7, 3))\n\ndef to_greyscale(im):\n    return 1 - np.dot(im[...,:3], [0.2989, 0.5870, 0.1140])\n\ngrey_img = to_greyscale(img)\n\naxarr[0].imshow(img)\naxarr[0].axis(\"off\")\naxarr[0].set(title = \"original\")\n\naxarr[1].imshow(grey_img, cmap = \"Greys\")\naxarr[1].axis(\"off\")\naxarr[1].set(title = \"greyscale\")\n\n[Text(0.5, 1.0, 'greyscale')]\n\n\n\n\n\nAbove is my favorite character from my favorite game, whose image has been grayscaled.\nAn \\(m \\times n\\) greyscale image needs \\(mn\\) numbers to represent it. If we use the SVD to compress the image using k components, we need to store the following matrices:\n\nThe \\(m \\times k\\) matrix \\(\\textbf{U}\\)\nThe \\(k \\times k\\) matrix \\(\\textbf{D}\\)\nThe \\(k \\times n\\) matrix \\(\\textbf{V}\\)\n\nThe total numbers needed to reconstruct the image is then \\(mk+k^2+kn\\). Using this, we get the formula for the amount of storage needed for a reconstruction as a fraction of the amount of storage needed for the original image:\n\\[ \\frac{mk+k^2+kn}{mn} \\times 100 \\%\\]\n\ndef svd_reconstruct(image, k):\n    # Compute the singular value decomposition of the image\n    U, sigma, V = np.linalg.svd(image, full_matrices=False)\n\n    # Truncate the matrices to keep only the first k singular values\n    Uk = U[:, :k]\n    Vk = V[:k, :]\n    \n    D = np.zeros_like(image,dtype=float) # matrix of zeros of same shape as A\n    D[:min(image.shape),:min(image.shape)] = np.diag(sigma) \n    Dk = D[:k, :k]\n    \n    # Reconstruct the image using the truncated matrices\n    reconstructed_image = Uk@Dk@Vk\n    return reconstructed_image\n\n\ndef svd_experiment(image, k_list):\n    # Presents a series of reconstructions of the same image for different values of k\n  \n    for k in k_list: # k_list is a list or array of k values\n        fig, axarr = plt.subplots()\n        hello = svd_reconstruct(image, k) # reconstruct image\n        m, n = image.shape\n        mn = m*n\n        stored = m*k + k**2 + k*n\n        axarr.imshow(hello, cmap = \"Greys\")\n        axarr.axis(\"off\")\n        axarr.set(title = str(k) + 'components, % storage = ' + str(round(stored/mn*100, 3)))\n\n    plt.show()\n\n\nhi = np.arange(5,81,5)\nsvd_experiment(grey_img, hi)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncompare_images(grey_img, svd_reconstruct(grey_img, 80))\n\n\n\n\nUsing only 6.5% of the initial storage for the original image, we can still make out what it is about: Razor eating some meat, out in the wild.\n\n\nPart 2: Spectral Community Detection\nWe will move on to identifying clusters in point cloud data sets. Below is a function that uses the Laplacian Spectral Clustering method to assign binary labels to nodes of a social network.\n\nimport networkx as nx\n\ndef spectral_clustering(G):\n    # Get the adjacency matrix of the graph\n    A = nx.adjacency_matrix(G).toarray()\n\n    # Compute the degree matrix of the graph\n    D = np.diag(np.sum(A, axis=1))\n\n    # Compute the Laplacian matrix of the graph\n    L = np.linalg.inv(D)@(D - A)\n\n    # Compute the eigenvectors of the Laplacian matrix\n    eigvals, eigvecs = np.linalg.eig(L)\n\n    # Sort the eigenvectors by their corresponding eigenvalues\n    sorted_indices = np.argsort(eigvals)\n    sorted_eigvecs = eigvecs[:, sorted_indices]\n\n    # Cluster the nodes based on the second smallest eigenvector\n    labels = sorted_eigvecs[:, 1] > 0\n\n    return labels*1\n\nWe will use the karate club social network.\n\nG = nx.karate_club_graph()\nlayout = nx.layout.fruchterman_reingold_layout(G)\nnx.draw(G, layout, with_labels=True, node_color = \"steelblue\")\n\n\n\n\nOur code predicts where each member would end up if the club got divided into 2. We will compare our code’s prediction to what actually happened.\n\nclubs = spectral_clustering(G)\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == 0 else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\") \n\nC:\\Users\\ledtr\\AppData\\Local\\Temp\\ipykernel_20396\\3045702461.py:5: FutureWarning: adjacency_matrix will return a scipy.sparse array instead of a matrix in Networkx 3.0.\n  A = nx.adjacency_matrix(G).toarray()\n\n\n\n\n\nAnd below is the actual division that happened.\n\nclubs = nx.get_node_attributes(G, \"club\")\nnx.draw(G, layout,\n        with_labels=True, \n        node_color = [\"orange\" if clubs[i] == \"Officer\" else \"steelblue\" for i in G.nodes()],\n        edgecolors = \"black\" # confusingly, this is the color of node borders, not of edges\n        ) \n\n\n\n\nOur code came pretty close to reality. The only difference is that while our code predicted that the eight member would end up in Officer’s club, they actually were in Mr. Hi’s club. A lot of factors can contribute to 8’s decision to stick with Mr. Hi: parental pressure (they may have said something like you have to stick by your mentor etc.), social hierarchy (8 was afraid of upsetting Mr. Hi), etc. With all these factors, I’m surprised that this predictive model was almost accurate. In order to say whether other social phenomena can be predicted this way, we need other social networks with binary outcomes such as this example. I tried digging on the Internet but every post about spectral clustering only mentions this example."
  },
  {
    "objectID": "posts/gebru/gebru.html",
    "href": "posts/gebru/gebru.html",
    "title": "Learning from Timnit Gebru",
    "section": "",
    "text": "Part 2: Dr. Gebru’s Talk “At” Middlebury\nWhen Dr. Gebru visited our class, she said that there is no incentive for corporate accountability to exist. The sole goal of businesses is to maximize profit regardless of the harm they cause. The incentive for accountability must therefore come from regulation and workers themselves, via unionizing etc.\nThere is no regulation on AI for businesses, however. Internal ethics departments in these businesses hold no power other than to shut down any external concerns about diversity etc. This means that the incentive for accountability must come from workers.\nOne might argue that AI scientists and engineers can create models that uplift rather than harm. However, the biggest investor in AI is the military, which means that whatever you build will go towards that, and scientists don’t have much say in what needs to be built. Furthermore, most scientists and engineers are aligned with power, and this leads them to create tools that persecute problems rather than prevent them.\nI agree with all of what Dr. Gebru said during this talk. I agree that there needs to be some sort of FDA to regulate technology and keep in check the harm that it can cause. But I doubt that there will be any kind of regulation as long as the military is involved. It makes little sense for the government to tell itself not to do such and such. The only instance where I see such an FDA to come into existence is near election day. Candidates may use this as a talking point for votes and then not carry any of it out because in the end the only goal of the US government is war.\nI also agree with Dr. Gebru’s comment that scientists align with power. I initially didn’t think she would say this because she is one of the scientists herself. Her talk is very consistent with what Lenin wrote about the intelligentsia in the struggle of the working class: university-educated people uphold the status quo and cannot be trusted. They are far removed from the struggle of the masses and sharpen the knife to the throat of the poor.\nIn the evening, Dr. Gebru talked about how AI companies exploit Kenyan workers to moderate traumatizing content for 2 dollars an hour. At the end of the talk, somebody asked if this would ultimately lead to a trickling down of technology, like the way people in poorer countries now have access to the iPhone etc, which is a good thing. Dr. Gebru disagreed with this, saying that some technology would still be withheld from these countries, and technology whose development is drenched in blood will never be ultimately good. This makes sense, because even though everybody has access to the iPhone, I’m pretty sure that Apple exploits workers in the global south and extracts resources from poor countries to maximize profit.\nDr. Gebru also talked about the eugenics intrinsic to AGI, how eugenics never goes away but instead keeps coming back in different forms that seems increasingly more progressive. I didn’t know about this before the talk, but I agree with everything she said because it makes perfect sense. It makes sense that AGI does not close the gap between the rich and poor but further widens it. The goal of AGI is to create a powerful model that can solve everything, like a chatbot for people too poor to afford healthcare etc., which sounds horrible. I always had my doubts about these new technologies because they are funded by the rich and designed in a very top-down sort of way. If the scientists involved in these projects are so far removed from the masses, they will surely develop models that persecute rather than uplift people.\nThere are actually models who are built in a bottom-up fashion, where the scientists involved aim to solve problems rather than disappear them. But they don’t get any funding because bigger, sloppy models take funding away from them.\nAt the end of the talk, Dr. Gebru mentions the co-option of safety, where people who are concerned about the evils of AGI keep talking about a distant, sci-fi sort of dystopia where robots become self-aware and destroy people. This diverts serious concern away from real, current disasters where people are being exploited, surveilled and destroyed by AI technology. This reminds me of that book The Handmaid’s Tale. After this book came out, every discourse about a woman’s autonomy over her body revolved about this book, which describes a distant, sci-fi, extreme scenario, rather than real people with real struggles in real countries that we know nothing about because The Handmaid’s Tale was the only talking point.\nAgain, I agreed with everything Dr. Gebru said, not because I am incapable of critical thinking. I critically thinkingly agreed with her. I believe that the most important thing to learn from her talk is that when you encounter new technology, you have to think really hard about who is funding it, for what purpose, who it is benefiting, and who it is destroying.\n\n\nPart 3: Reflect on the Process\nI didn’t expect to enjoy learning from Dr. Gebru this much. I usually don’t like attending talks by guest scientists because they’re usually too technical for me to understand. Dr. Gebru took a very human approach with this talk: she constructed her arguments around real people, real issues that everybody understands. She does not concern us with arguments about some theoretical work. I like the fact that everything she said resonates with what I read from Marx and Lenin, but she didn’t bring up anything by them. I think that if somebody keeps bringing up quotations from books to prove that they’re right, then they don’t really know what they’re talking about. Dr. Gebru is very empirical with her work; she grounds her talking points in the suffering of the people exploited by tech companies. That’s what makes her talks so engaging and memorable."
  },
  {
    "objectID": "posts/project_blogpost/project.html",
    "href": "posts/project_blogpost/project.html",
    "title": "Determining Race from Chest X-Rays",
    "section": "",
    "text": "The GitHub repository for the project code can be found here."
  },
  {
    "objectID": "posts/project_blogpost/project.html#does-race-exist",
    "href": "posts/project_blogpost/project.html#does-race-exist",
    "title": "Determining Race from Chest X-Rays",
    "section": "Does Race Exist?",
    "text": "Does Race Exist?\nWhether race exists as a biological phenomenon, and not as a social construct, is a hotly debated issue. As Cerdena et. al. note, “race was developed as a tool to divide and control populations worldwide. Race is thus a social and power construct with meanings that have shifted over time to suit political goals, including to assert biological inferiority of dark-skinned populations” [1].\nOne justification for the biological reality of races is based on the assumption that different races have distinct genetics from one another, and can be fit into genetic groups. However, Maglo et. al. note that humans are not distinct by evolutionary criteria and genetic similarities between “human races, understood as continental clusters, have no taxonomic meaning”, with there being “tremendous diversity within groups” [2]. Whether race defines a genetic profile is therefore unclear at best, with correlations between race and disease being confounded by variables such as the association between race and socioeconomic variables (NEEDS CITATION)."
  },
  {
    "objectID": "posts/project_blogpost/project.html#what-is-race-based-medicine",
    "href": "posts/project_blogpost/project.html#what-is-race-based-medicine",
    "title": "Determining Race from Chest X-Rays",
    "section": "What is Race-based Medicine?",
    "text": "What is Race-based Medicine?\nIt is possible that some may be interested in using this algorithm to deduce the race of an individual and use this as part of medical decisions. There are some correlations between disease prevalence and race. Maglo et. al. note that “Recent studies showed that ancestry mapping has been successfully applied for disease in which prevalence is significantly different between the ancestral populations to identify genomic regions harboring diseases susceptibility loci for cardiovascular disease (Tang et al., 2005), multiple sclerosis (Reich et al., 2005), prostate cancer (Freedman et al., 2006), obesity (Cheng et al., 2009), and asthma (Vergara et al., 2009)” [2].\nThese practices would be characteristic of race-based medicine. As Cerdena et. al argue, this is “the system by which research characterizing race as an essential, biological variable, [which] translates into clinical practice, leading to inequitable care” [1]. Notably, then, race-based medicine has come under heavy criticism."
  },
  {
    "objectID": "posts/project_blogpost/project.html#the-harms-of-race-based-medicine",
    "href": "posts/project_blogpost/project.html#the-harms-of-race-based-medicine",
    "title": "Determining Race from Chest X-Rays",
    "section": "The Harms of Race-based Medicine",
    "text": "The Harms of Race-based Medicine\nAs stated above, race is not an accurate proxy for genetics. Cerdena et al. note that in medical practices, race is used as an inaccurate guideline for medical care: “Black patients are presumed to have greater muscle mass …On the basis of the understanding that Asian patients have higher visceral body fat than do people of other races, they are considered to be at risk for diabetes at lower body-mass indices” [1]. As they note, race-based medicine can be founded more on racial stereotypes and generalizations rather than.\nMoreover, race-based medicine can lead to ineffective treatements. Apeles summarizes the results of a study on race-based prescriptions for Black patients for high blood pressure. While this study demonstrates that alternative prescriptions for Black patients with high blood pressure have been shown to be ineffective, “Practice guidelines have long recommended that Black patients with high blood pressure and no comorbidities be treated initially with a thiazide diuretic or a calcium channel blocker (CCB) instead of an angiotensin converting enzyme inhibitor (ACEI) and/or angiotensin receptor blocker (ARB). By contrast, non-Black patients can be prescribed any of those medicines regardless of comorbidities.” In addition, the authors of the study found that “other factors may be more important than considerations of race, such as dose, the addition of second or third drugs, medication adherence, and dietary and lifestyle interventions. Follow-up care was important, and the Black patients who had more frequent clinical encounters tended to have better control of their blood pressure.”\nIn addition, [Vyas et. al] argue that race is ill-suited as a correction factor for medical algorithms. As they found, algorithms as the American Heart Association (AHA) Get with the Guidelines–Heart Failure Risk Score, which predicts the likelihood of death from heart failure, the Vaginal Birth after Cesarean (VBAC), which predicts the risk of labor for someone with a previous cesarean section, and STONE score, which predicts the likelihood of kidney stones in patients with flank pain, all used race to change their predictions of the likelihood or morbidities. However, they find that these algorithms were not sufficiently evidence based as “Some algorithm developers offer no explanation of why racial or ethnic differences might exist. Others offer rationales, but when these are traced to their origins, they lead to outdated, suspect racial science or to biased data”. Using race can then discourage racial minorities from receiving the proper treatment based on their scores, exacerbating already existing problems of unequal health outcomes."
  },
  {
    "objectID": "posts/project_blogpost/project.html#conclusion",
    "href": "posts/project_blogpost/project.html#conclusion",
    "title": "Determining Race from Chest X-Rays",
    "section": "Conclusion",
    "text": "Conclusion\nSo it is clear that anyone who intends to use race for diagnosis could harm racial minority groups. Race inherently is a complex social and economic phenomenon and cannot be said to be a clear biological variable. Hence anyone intending to use or create algorithms will run the risk of creating dangerous biases in treatment; ones that could worsen the existing disparities in care for vulnerable populations."
  }
]