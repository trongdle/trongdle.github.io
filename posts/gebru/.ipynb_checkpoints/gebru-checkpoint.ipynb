{
 "cells": [
  {
   "cell_type": "raw",
   "id": "5bd98fee-93b9-4269-8242-b01328dd38a9",
   "metadata": {},
   "source": [
    "---\n",
    "title: Learning from Timnit Gebru\n",
    "author: Trong Le\n",
    "date: '2023-04-18'\n",
    "image: \"gebru.jpg\"\n",
    "description: \"Preparation for Dr. Gebru's talk at Middlebury College.\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b77701-e4de-451b-876c-55003251cbab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 1: Questions for Dr. Gebru\n",
    "\n",
    "__Introduction__ \n",
    "\n",
    "On April 24, in class, we will have a chance to have a virtual Q&A session with Dr. Gebru about her recent work in AI ethics. In the evening of April 24th, at 7pm in Hillcrest 103, Dr. Gebru will give a talk on “Eugenics and the Promise of Utopia through Artificial General Intelligence.”\n",
    "\n",
    "Dr. Gebru has gained international recognition due to her groundbreaking research and contributions to the field. Her work has been instrumental in highlighting the impact of AI on marginalized communities, exposing bias in machine learning, and emphasizing the importance of ethical considerations in the development of AI systems. Dr. Gebru's expertise in the field is highly sought after, and her presence at Middlebury College is a significant event that is sure to provide unparalleled insights and inspire important conversations about the evils that artificial intelligence is capable of. \n",
    "\n",
    "__Dr. Gebru's Talk__\n",
    "\n",
    "In 2020, Dr. Gebru gave a talk as part of a __[Tutorial on Fairness, Accountability, Transparency, and Ethics (FATE) in Computer Vision](https://sites.google.com/view/fatecv-tutorial/home?authuser=0)__  at the conference on Computer Vision and Pattern Recognition 2020. The recording of her talk can be found __[here](https://www.youtube.com/watch?v=0sBE5OyD7fk&t=802s)__.\n",
    "\n",
    "During her talk, Dr. Gebru spoke at length about the intrusive and destructive use of artificial intelligence on marginalized groups. For example, facial recognition was used to match protestors to their social media to ensure arrests. Allegedly, with the help of robot police, nobody is safe from profiling. That robot police hate minorities as much as human police is a result of bias in datasets used to train computer vision systems. Dr. Gebru noted that many of these datasets were collected, trained, sold, and used by white people and corporations, which unavoidably led to ineffective predictive models for minorities. She argued that computer vision systems are not neutral technologies but are instead shaped by the social, cultural, and political contexts in which they are used. As such, it is important to consider how these systems may impact different communities and to ensure that they are designed and used in ways that are both equitable and just.\n",
    "\n",
    "Another issue with these models is that even if a model works equally well on anyone (i.e. no bias), it can still be bad. For example, if the AI model predicts that a user is female, it will direct advertisements about things that women are encouraged to use, thus reinforcing existing stereotypes and discriminations. In a broader context, Dr. Gebru noted that by making a model 'fairer', we would only make the institutions that use the model colder and more punitive.\n",
    "\n",
    "Dr. Gebru also argued that a 'good' model could still inflict harm on minorities if it falls into the wrong hands. This is because researchers in the field over-abstract their work, up to the point where humans in a study are disappeared into mathematical equations. It is important to consider that when working with data, we see subjects and objects, but on both sides are human beings. The most effective way to stop AI from inflicting harm is for scientists to actively prevent marginalization when they are working on their models.\n",
    "\n",
    "tl;dr\n",
    "\n",
    "The primary takeaway from Dr. Gebru's talk is that computer vision systems are not neutral technologies but are instead shaped by the social, cultural, and political contexts in which they are used, and as such, it is important to consider the potential biases and impact of these systems on marginalized communities.\n",
    "\n",
    "__Questions for Dr. Gebru__\n",
    "\n",
    "In Dr. Gebru's view, what role should policymakers play in addressing issues of bias and fairness in the development and deployment of computer vision systems, and what steps can they take to ensure that these systems are designed and used in ways that are both equitable and just?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ml-0451] *",
   "language": "python",
   "name": "conda-env-ml-0451-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
