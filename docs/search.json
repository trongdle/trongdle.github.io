[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog I do not understand"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "In this blog post, I will implement least-squares linear regression, and experiment with LASSO regularization for overparameterized problems..\n\n\n\n\n\n\nMar 23, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nClassifying Palmer Penguins with Sklearn.\n\n\n\n\n\n\nMar 7, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nOptimization algorithms based on the gradients of functions.\n\n\n\n\n\n\nFeb 26, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nIn this blog post, I will implement the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets.\n\n\n\n\n\n\nFeb 19, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "beta*(w_tilda - w_tilda_prev) # this is w_{k+1} w_tilda_prev = w_tilda # this is w_{k-1} w_tilda = w_tilda_new # this is w_{k}\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/gradient/gradient.html",
    "href": "posts/gradient/gradient.html",
    "title": "Gradient Descent",
    "section": "",
    "text": "Below is our data, which is not linearly separable.\n\nfrom solutions import LogisticRegression\nfrom sklearn.datasets import make_blobs\nfrom matplotlib import pyplot as plt\nimport numpy as np\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 500, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n1. Gradient Descent\nThe gradient descent algorithm is implemented when the LogisticRegression.fit(X, y) is called. \\(\\textbf{X}\\) is the feature matrix, and \\(\\textbf{y}\\) is the target vector, as we know from the perceptron blog post. We need to find a weight vector that can minimize the logistic loss on \\(\\textbf{X}\\) and \\(\\textbf{y}\\) by updating the weight in a loop. At every loop:\n\ncompute the gradient descent of the logistic loss, which is given by \\[\\nabla L(\\textbf{w}) = \\frac{1}{n} \\sum^n_{i=1} (\\sigma (\\langle \\textbf{w}, \\textbf{x}_i \\rangle), y_i) \\textbf{w}_i, \\] where \\(\\sigma(z)\\) denotes the sigmoid function.\nupdate the weight until the logistic loss does not change \\[\\textbf{w}^{(t+1)} = \\textbf{w}^{(t)} - \\alpha \\nabla L(\\textbf{w}^{(t)})\\]\n\n\n# fit the model\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = 10, max_epochs = 1000)\n\n# inspect the fitted value of w\nLR.w \n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfigy, axy = plt.subplots()\naxy.scatter(X[:,0], X[:,1], c = y)\ndraw_line(LR.w, -2, 2)\naxy.set_xlabel(\"Feature 1\")\naxy.set_ylabel(\"Feature 2\")\n\nfigx, axv = plt.subplots()\naxv.plot(LR.loss_history)\naxv.set_xlabel(\"Iteration\")\naxv.set_ylabel(\"Empirical Risk\")\nplt.show()\n\n\n\n\n\n\n\nIn this case, the learning rate was set very high to 10, which caused the algorithm to converge too soon and leave the logistic loss at 0.30. As we change the learning rate to 0.1, we can see that the loss fell to 0.27.\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha=0.1, max_epochs=1000)\n\nfigy, axy = plt.subplots()\naxy.scatter(X[:,0], X[:,1], c = y)\ndraw_line(LR.w, -2, 2)\naxy.set_xlabel(\"Feature 1\")\naxy.set_ylabel(\"Feature 2\")\n\nfigx, axv = plt.subplots()\naxv.plot(LR.loss_history)\naxv.set_xlabel(\"Iteration\")\naxv.set_ylabel(\"Empirical Risk\")\nplt.show()\n\n\n\n\n\n\n\n\n\n2. Stochastic Gradient Descent\nThe stochastic gradient descent algorithm is implemented by the LogisticRegression.fit_stochastic(X, y) method. In this algorithm, instead of computing the complete gradient as in part 1, we compute a stochastic gradient by picking a random subset \\(S \\subseteq [n] = \\{1, ..., n\\}\\) and computing\n\\[\\nabla_S L(\\textbf{w}) = \\frac{1}{|S|} \\sum_{i \\in S} \\nabla (\\sigma (\\langle \\textbf{w}, \\textbf{x}_i \\rangle), y_i)\\textbf{x}_i \\]\nThe size of \\(S\\) is the batch size, which we can refer to as \\(k\\). The batch gradient descent is performed as follows: 1. Shuffle the points randomly. 2. Pick the first \\(k\\) random points, compute the stochastic gradient, and then perform an update. 3. Pick the next \\(k\\) random points and repeat.. 4. When we have gone through all \\(n\\) points, reshuffle them all randomly and proceed again.\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 3\nX, y = make_blobs(n_samples = 200, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this experiment, we can see how the stochastic gradient descent algorithm can outperform the original gradient descent method.\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\n\nLR = LogisticRegression()\nLR.fit(X, y, alpha = .1, max_epochs = 100)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"gradient\")\nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Empirical Risk\")\nlegend = plt.legend() \n\n\n\n\nThe stochastic gradient method used only 25 loops, while the gradient method still had not converged at loop 100.\n\n\n3. Stochastic Gradient Descent with Momentum\nThis only difference in this method comes from the momentum factor \\(\\beta\\). The weight update is given by\n\\[\\textbf{w}^{(t+1)} = \\textbf{w}^{(t)} - \\alpha \\nabla L(\\textbf{w}^{(t)}) + \\beta (\\textbf{w}^{(t)}-\\textbf{w}^{(t-1)})\\]\nThe idea here is that if the previous weight update was good, we may want to continue moving along this direction. To test how the momentum can help with accelerating convergence, we choose a small sample size of 150, distributed among 10 features.\n\nnp.seterr(all='ignore') \n\n# make the data\np_features = 11\nX, y = make_blobs(n_samples = 150, n_features = p_features - 1, centers = [(-1, -1), (1, 1)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = True, \n                  batch_size = 10, \n                  alpha = .1) \n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient (momentum)\")\n\n\nLR = LogisticRegression()\nLR.fit_stochastic(X, y, \n                  m_epochs = 100, \n                  momentum = False, \n                  batch_size = 10, \n                  alpha = .1)\n\nnum_steps = len(LR.loss_history)\nplt.plot(np.arange(num_steps) + 1, LR.loss_history, label = \"stochastic gradient\")\nlegend = plt.legend() \nplt.xlabel(\"Iteration\")\nplt.ylabel(\"Empirical Risk\")\nplt.show()\n\n\n\n\nThe momentum algorithm converges in less than a quarter of the loops it takes the other method."
  },
  {
    "objectID": "posts/linear-regression/linear-regression.html",
    "href": "posts/linear-regression/linear-regression.html",
    "title": "Implementing Linear Regression",
    "section": "",
    "text": "The optimization for linear regression can be implemented by using my source code linear.py, which can be found HERE. In this code, loss optimization is implemented in two ways: using gradient descent, and using the optimal weight formula. We will check if this code is implemented correctly.\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\ndef pad(X):\n    return np.append(X, np.ones((X.shape[0], 1)), 1)\n\ndef LR_data(n_train = 100, n_val = 100, p_features = 1, noise = .1, w = None):\n    if w is None: \n        w = np.random.rand(p_features + 1) + .2\n    \n    X_train = np.random.rand(n_train, p_features)\n    y_train = pad(X_train)@w + noise*np.random.randn(n_train)\n\n    X_val = np.random.rand(n_val, p_features)\n    y_val = pad(X_val)@w + noise*np.random.randn(n_val)\n    \n    return X_train, y_train, X_val, y_val\n\n\nn_train = 100\nn_val = 100\np_features = 1\nnoise = 0.2\n\n# create some data\nX_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features, noise)\n\n# plot it\nfig, axarr = plt.subplots(1, 2, sharex = True, sharey = True)\naxarr[0].scatter(X_train, y_train)\naxarr[1].scatter(X_val, y_val)\nlabs = axarr[0].set(title = \"Training\", xlabel = \"x\", ylabel = \"y\")\nlabs = axarr[1].set(title = \"Validation\", xlabel = \"x\")\nplt.tight_layout()\n\n\n\n\n\nfrom linear import LinearRegression\n\nLR = LinearRegression()\nLR.fit_analytic(X_train, y_train) # I used the analytical formula as my default fit method\n\nprint(f\"Training score = {LR.score(X_train, y_train).round(4)}\")\nprint(f\"Validation score = {LR.score(X_val, y_val).round(4)}\")\n\nTraining score = 0.6311\nValidation score = 0.6072\n\n\n\nLR.w.reshape(-1,1)\n\narray([[1.05653571],\n       [0.96877292]])\n\n\n\nLR2 = LinearRegression()\ny2 = y_train.reshape(-1,1)\nLR2.fit_gradient(X_train, y2, alpha=0.01)\nLR2.w\n\narray([[1.10993122],\n       [0.94029021]])\n\n\nSo the weights in both cases are very similar, which means that the gradient method is behaving as it should.\nThe score also increases monotonically in each iteration.\n\nplt.plot(LR2.score_history)\nlabels = plt.gca().set(xlabel = \"Iteration\", ylabel = \"Score\")"
  },
  {
    "objectID": "posts/linear-regression/linear-regression.html#experiments",
    "href": "posts/linear-regression/linear-regression.html#experiments",
    "title": "Implementing Linear Regression",
    "section": "Experiments",
    "text": "Experiments\nNow that we have established that our LinearRegression module is behaving okay, we can move on to experiments, where we increase the number of features and observe the change in validation scores.\n\nn_train = 100\nn_val = 100\np_features = np.arange(1,100)\nnoise = 0.2\ntrain_score = []\ntest_score = []\n\n# create some data\nfor i in range(len(p_features)):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features[i], noise)\n    LR = LinearRegression()\n    LR.fit_analytic(X_train, y_train)\n    train_score.append(LR.score(X_train, y_train).round(4))\n    test_score.append(LR.score(X_val, y_val).round(4))\n\nplt.plot(p_features, train_score, label = 'Training score')\nplt.plot(p_features, test_score, label = 'Validation score')\nplt.legend()\nplt.xlabel('Number of features')\nplt.ylabel('Score')\nplt.grid()\nplt.show()\n\n\n\n\nAs the number of features increases, the training score approaches 100% while validation score becomes noisier. This demonstrates the effect of overfitting: because the machine is trained to account for every little detail of the training dataset, it becomes too inflexible to work with unseen data."
  },
  {
    "objectID": "posts/linear-regression/linear-regression.html#lasso-regularization",
    "href": "posts/linear-regression/linear-regression.html#lasso-regularization",
    "title": "Implementing Linear Regression",
    "section": "LASSO Regularization",
    "text": "LASSO Regularization\n\nfrom sklearn.linear_model import Lasso\nL = Lasso(alpha = 0.001)\n\n\nn_train = 100\nn_val = 100\np_features = np.arange(1,100)\nnoise = 0.2\ntrain_score = []\ntest_score = []\n\n# create some data\nfor i in range(len(p_features)):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features[i], noise)\n    L = Lasso(alpha = 0.001)\n    L.fit(X_train, y_train)\n    train_score.append(L.score(X_train, y_train).round(4))\n    test_score.append(L.score(X_val, y_val).round(4))\n\nplt.plot(p_features, train_score, label = 'Training score')\nplt.plot(p_features, test_score, label = 'Validation score')\nplt.legend()\nplt.xlabel('Number of features')\nplt.ylabel('Score')\nplt.grid()\nplt.show()\n\n\n\n\nIt seems that even when the number of features used is large, the validation score isn’t affected as badly as in the case of pure linear regression. Now we change the strength of regularization to see what changes.\n\ntrain_score = []\ntest_score = []\n\n# create some data\nfor i in range(len(p_features)):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features[i], noise)\n    L = Lasso(alpha = 0.01)\n    L.fit(X_train, y_train)\n    train_score.append(L.score(X_train, y_train).round(4))\n    test_score.append(L.score(X_val, y_val).round(4))\n\nplt.plot(p_features, train_score, label = 'Training score')\nplt.plot(p_features, test_score, label = 'Validation score')\nplt.legend()\nplt.xlabel('Number of features')\nplt.ylabel('Score')\nplt.grid()\nplt.show()\n\n\n\n\n\ntrain_score = []\ntest_score = []\n\n# create some data\nfor i in range(len(p_features)):\n    X_train, y_train, X_val, y_val = LR_data(n_train, n_val, p_features[i], noise)\n    L = Lasso(alpha = 0.0005)\n    L.fit(X_train, y_train)\n    train_score.append(L.score(X_train, y_train).round(4))\n    test_score.append(L.score(X_val, y_val).round(4))\n\nplt.plot(p_features, train_score, label = 'Training score')\nplt.plot(p_features, test_score, label = 'Validation score')\nplt.legend()\nplt.xlabel('Number of features')\nplt.ylabel('Score')\nplt.grid()\nplt.show()\n\n\n\n\nIt seems that when the strength of regularization is low, overfitting is less bad."
  },
  {
    "objectID": "posts/penguins/penguins.html",
    "href": "posts/penguins/penguins.html",
    "title": "Penguins",
    "section": "",
    "text": "import pandas as pd\n\ntrain_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\nspecies = [s.split()[0] for s in le.classes_]\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\nX_train\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n      Flipper Length (mm)\n      Body Mass (g)\n      Delta 15 N (o/oo)\n      Delta 13 C (o/oo)\n      Island_Biscoe\n      Island_Dream\n      Island_Torgersen\n      Stage_Adult, 1 Egg Stage\n      Clutch Completion_No\n      Clutch Completion_Yes\n      Sex_FEMALE\n      Sex_MALE\n    \n  \n  \n    \n      1\n      45.1\n      14.5\n      215.0\n      5000.0\n      7.63220\n      -25.46569\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      2\n      41.4\n      18.5\n      202.0\n      3875.0\n      9.59462\n      -25.42621\n      0\n      0\n      1\n      1\n      0\n      1\n      0\n      1\n    \n    \n      3\n      39.0\n      18.7\n      185.0\n      3650.0\n      9.22033\n      -26.03442\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      4\n      50.6\n      19.4\n      193.0\n      3800.0\n      9.28153\n      -24.97134\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      5\n      33.1\n      16.1\n      178.0\n      2900.0\n      9.04218\n      -26.15775\n      0\n      1\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      269\n      41.1\n      17.5\n      190.0\n      3900.0\n      8.94365\n      -26.06943\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      270\n      45.4\n      14.6\n      211.0\n      4800.0\n      8.24515\n      -25.46782\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n    \n      271\n      36.2\n      17.2\n      187.0\n      3150.0\n      9.04296\n      -26.19444\n      0\n      0\n      1\n      1\n      1\n      0\n      1\n      0\n    \n    \n      272\n      50.0\n      15.9\n      224.0\n      5350.0\n      8.20042\n      -26.39677\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n    \n    \n      273\n      48.2\n      14.3\n      210.0\n      4600.0\n      7.68870\n      -25.50811\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n    \n  \n\n256 rows × 14 columns\n\n\n\nWe need to find the combination of features with which we can determine the species of a penguin the fastest. We can accomplish this by running an exhaustive search of all the features contained in this data set. At each iteration, we use the LogisticRegression module to test the score on the current set of features.\n\nfrom itertools import combinations\nfrom sklearn.linear_model import LogisticRegression\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Clutch Completion\", \"Sex\", 'Island']\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Flipper Length (mm)', 'Body Mass (g)']\nbest_score = 0\nbest_col = 0\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = list(pair) + qual_cols\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    if best_score < LR.score(X_train[cols], y_train):\n        best_col = cols\n        best_score = LR.score(X_train[cols], y_train)\nprint(best_col)\n\n['Culmen Length (mm)', 'Culmen Depth (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\n\nWe can visualize the training data by using Seaborn and apply it on the most efficient combination of features that we have just found.\n\nimport seaborn as sns\n\n# Apply the default theme\nsns.set_theme()\n\n# Create a visualization\nsns.relplot(\n    data=train,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", col=\"Island\", hue ='Species')\n\n<seaborn.axisgrid.FacetGrid at 0x1f4f9fce4c0>\n\n\n\n\n\nWe can see that the training data is linearly separable. This means that the score on our logistic regression should be 100%.\n\nLR = LogisticRegression()\nLR.fit(X_train[best_col], y_train)\nLR.score(X_train[best_col], y_train)\n\n1.0\n\n\nAnd it is 100%! Next we will try to make some interesting tables from this. First, we will divide the data set by island.\n\ntrain.groupby('Island')[['Culmen Length (mm)', 'Culmen Depth (mm)']].aggregate(min).round(2)\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Island\n      \n      \n    \n  \n  \n    \n      Biscoe\n      34.5\n      13.1\n    \n    \n      Dream\n      32.1\n      15.5\n    \n    \n      Torgersen\n      33.5\n      16.6\n    \n  \n\n\n\n\n\ntrain.groupby('Island')[['Culmen Length (mm)', 'Culmen Depth (mm)']].aggregate(max).round(2)\n\n\n\n\n\n  \n    \n      \n      Culmen Length (mm)\n      Culmen Depth (mm)\n    \n    \n      Island\n      \n      \n    \n  \n  \n    \n      Biscoe\n      59.6\n      21.1\n    \n    \n      Dream\n      55.8\n      21.2\n    \n    \n      Torgersen\n      46.0\n      21.5\n    \n  \n\n\n\n\nThese tables give the range of culmen length and depth of the penguins on each island. For example, on Biscoe island, the culmen length range is 34.5 mm - 59.6 mm. This means that this island must have Adelie penguins on it because Adelie penguins have the shortest culmens. The 59.6 mm maximum suggests that the other species may be Gentoo or Chinstrap. The tight length range 33.5 mm - 46.0 mm suggests that there are only Adelie penguins on Torgensen island. By making quantitative comparisons like this, we can linearly separate our data.\n\nfrom matplotlib.patches import Patch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\n\nplot_regions(LR, X_train[best_col], y_train)\n\n\n\n\nThese 3 graphs correspond to the 3 islands: Biscoe, Dream and Torgensen.\nNow that we have established that “Culmen Length”, “Culmen Depth” and “Island” make the best feature combination, we will test this out on new data.\n\ntest_url = \"https://raw.githubusercontent.com/middlebury-csci-0451/CSCI-0451/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\n\nsns.relplot(\n    data=test,\n    x=\"Culmen Length (mm)\", y=\"Culmen Depth (mm)\", col=\"Island\", hue ='Species')\n\n<seaborn.axisgrid.FacetGrid at 0x1f4f8b00d00>\n\n\n\n\n\nThe data set, as organized by our feature combination, is again linearly separable. Does this guarantee a score of 100%?\n\nX_test, y_test = prepare_data(test)\nLR.score(X_test[best_col], y_test)\n\n1.0\n\n\nSo it does!"
  },
  {
    "objectID": "posts/perceptron/perceptron.html",
    "href": "posts/perceptron/perceptron.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "The primary method of the Perceptron class is Perceptron.fit(X, y). \\(\\textbf{X} \\in \\mathbb{R}^{n\\times p}\\) is a matrix of predictor variables, with \\(n\\) observations and \\(p\\) features, and \\(\\textbf{y} \\in {0, 1}^n\\) is a vector of binary labels. When the method Perceptron.fit(X, y) is called, the perceptron in question will get an instance variable of weights called w, and a history of the perceptron’s accuracy scores during the runtime of the function, called history.\nBefore updating the perceptron’s weight, we rewrite the problem with \\(\\tilde{\\textbf{X}} = [ \\textbf{X}, \\textbf{1} ]\\) and \\(\\tilde{\\textbf{w}} = (\\textbf{w}, - b)\\) to account for the bias. The perceptron algorithm is implemented as follows:\n\nWe start by assigning a random weight vector \\(\\tilde{\\textbf{w}}^{(0)}\\).\nUntil the perceptron achieves perfect accuracy (or until termination), in each time step t:\n\nPick a random point index \\(i\\) in \\(\\textbf{y}\\)\nUpdate the weight: \\[\\tilde{\\textbf{w}}^{(t+1)} = \\tilde{\\textbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\textbf{w}}^{(t)}, \\tilde{x}_i \\rangle < 0) \\tilde{y}_i \\tilde{x}_i,\\] where \\(\\tilde{y}_i = 2y_i -1\\), which takes on values of -1 and 1 instead of 0 and 1. The Boolean function in this equation checks whether or not the weight needs an update. It does not update when \\(\\tilde{y}_i \\langle \\tilde{\\textbf{w}}^{(t)}, \\tilde{x}_i \\rangle \\geq 0\\), which means that the prediction value matches the real value.\n\n\nWe will use this algorithm to conduct a few experiments, and confirm that if a set of data is linearly separable, this algorithm will separate the data.\n\nExperiment 1\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this experiment, our data have 2 features, and we can see that they are linearly separable. First, we import the Perceptron class from the code that I wrote. After calling the Perceptron.fit(X, y) method, we will find a weight vector \\(\\tilde{\\textbf{w}}\\) that describes the separating line.\n\nfrom perceptron import Perceptron\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nGiven this set of linearly separable data, over time, our perceptron was able to separate the data with 100% accuracy. This was achieved within under 200 time steps.\n\n\nExperiment 2\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (0,0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this experiment, the data overlap each other, which makes them linearly inseparable. If we implement the perceptron algorithm in this case, the method Perceptron.fit(X, y) until it terminates, but its accuracy will never achieve 100%.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nBecause the perceptron algorithm never reached an accuracy of 100%, it was iterated 1000 times until it terminated. This is one of the limitations of the perceptron algorithm.\n\n\nExperiment 3\n\nnp.random.seed(12345)\n\nn = 100\np_features = 8\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = 7)\n\nIn this case, we consider 7 features. Because the data set is no longer 2-dimensional, visualization is not possible. There are 7 centers spread randomly in the \\(\\mathbb{R}^7\\) space, each of which is surrounded by a cluster of one feature.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nThe history of this perceptron indicates that this data set is not linearly separable because the accuracy of the perceptron never reaches 100%, and the algorithm runs for too long.\n\n\nRuntime Complexity\nWe are concerned with the runtime complexity of a single iteration of the perceptron algorithm update. We assume that the relevant operations are addition and multiplication. According to the update function, the operation that takes up the most time is the dot product between \\(\\tilde{\\textbf{w}}\\) and \\(\\tilde{\\textbf{x}}_i\\). Each of these terms has length \\(p+1\\) (\\(p\\) is the number of features). This is a sum of \\(p+1\\) products, which means that this update function has a time complexity of \\(\\mathcal{O}(p)\\). This depends only on the number of features, and not the number of data points."
  }
]