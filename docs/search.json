[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog I do not understand"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "In this blog post, I will implement the perceptron algorithm using numerical programming and demonstrate its use on synthetic data sets.\n\n\n\n\n\n\nFeb 19, 2023\n\n\nTrong Le\n\n\n\n\n\n\n  \n\n\n\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/perceptron/perceptron.html",
    "href": "posts/perceptron/perceptron.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "The primary method of the Perceptron class is Perceptron.fit(X, y). \\(\\textbf{X} \\in \\mathbb{R}^{n\\times p}\\) is a matrix of predictor variables, with \\(n\\) observations and \\(p\\) features, and \\(\\textbf{y} \\in {0, 1}^n\\) is a vector of binary labels. When the method Perceptron.fit(X, y) is called, the perceptron in question will get an instance variable of weights called w, and a history of the perceptron’s accuracy scores during the runtime of the function, called history.\nBefore updating the perceptron’s weight, we rewrite the problem with \\(\\tilde{\\textbf{X}} = [ \\textbf{X}, \\textbf{1} ]\\) and $ = (, 1)$ to account for the bias. The perceptron algorithm is implemented as follows:\n\nWe start by assigning a random weight vector \\(\\tilde{\\textbf{w}}^{(0)}\\).\nUntil the perceptron achieves perfect accuracy (or until termination), in each time step t:\n\nPick a random point index \\(i\\) in \\(\\textbf{y}\\)\nUpdate the weight: \\[\\tilde{\\textbf{w}}^{(t+1)} = \\tilde{\\textbf{w}}^{(t)} + \\mathbb{1}(\\tilde{y}_i \\langle \\tilde{\\textbf{w}}^{(t)}, \\tilde{x}_i \\rangle < 0) \\tilde{y}_i \\tilde{x}_i,\\] where \\(\\tilde{y}_i = 2y_i -1\\), which takes on values of -1 and 1 instead of 0 and 1. The Boolean function in this equation checks whether or not the weight needs an update. It does not update when \\(\\tilde{y}_i \\langle \\tilde{\\textbf{w}}^{(t)}, \\tilde{x}_i \\rangle \\geq 0\\), which means that the prediction value matches the real value.\n\n\nWe will use this algorithm to conduct a few experiments, and confirm that if a set of data is linearly separable, this algorithm will separate the data.\n\nExperiment 1\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n\nfrom sklearn.datasets import make_blobs\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (1.7, 1.7)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this experiment, our data have 2 features, and we can see that they are linearly separable. First, we import the Perceptron class from the code that I wrote. After calling the Perceptron.fit(X, y) method, we will find a weight vector \\(\\tilde{\\textbf{w}}\\) that describes the separating line.\n\nfrom perceptron import Perceptron\n\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\n\ndef draw_line(w, x_min, x_max):\n  x = np.linspace(x_min, x_max, 101)\n  y = -(w[0]*x + w[2])/w[1]\n  plt.plot(x, y, color = \"black\")\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nGiven this set of linearly separable data, over time, our perceptron was able to separate the data with 100% accuracy. This was achieved within under 200 time steps.\n\n\nExperiment 2\n\nnp.random.seed(12345)\n\nn = 100\np_features = 3\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = [(-1.7, -1.7), (0,0)])\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\nIn this experiment, the data overlap each other, which makes them linearly inseparable. If we implement the perceptron algorithm in this case, the method Perceptron.fit(X, y) until it terminates, but its accuracy will never achieve 100%.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\n\nfig = plt.scatter(X[:,0], X[:,1], c = y)\nfig = draw_line(p.w, -2, 2)\n\nxlab = plt.xlabel(\"Feature 1\")\nylab = plt.ylabel(\"Feature 2\")\n\n\n\n\n\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nBecause the perceptron algorithm never reached an accuracy of 100%, it was iterated 1000 times until it terminated. This is one of the limitations of the perceptron algorithm.\n\n\nExperiment 3\n\nnp.random.seed(12345)\n\nn = 100\np_features = 8\n\nX, y = make_blobs(n_samples = 100, n_features = p_features - 1, centers = 7)\n\nIn this case, we consider 7 features. Because the data set is no longer 2-dimensional, visualization is not possible. There are 7 centers spread randomly in the \\(\\mathbb{R}^7\\) space, each of which is surrounded by a cluster of one feature.\n\np = Perceptron()\np.fit(X, y, max_steps = 1000)\nfig = plt.plot(p.history)\nxlab = plt.xlabel(\"Iteration\")\nylab = plt.ylabel(\"Accuracy\")\n\n\n\n\nThe history of this perceptron indicates that this data set is not linearly separable because the accuracy of the perceptron never reaches 100%, and the algorithm runs for too long.\n\n\nRuntime Complexity\nWe are concerned with the runtime complexity of a single iteration of the perceptron algorithm update. We assume that the relevant operations are addition and multiplication. According to the update function, the operation that takes up the most time is the dot product between \\(\\tilde{\\textbf{w}}\\) and \\(\\tilde{\\textbf{x}}_i\\). Each of these terms has length \\(p+1\\) (\\(p\\) is the number of features). This is a sum of \\(p+1\\) products, which means that this update function has a time complexity of \\(\\mathcal{O}(p)\\). This depends only on the number of features, and not the number of data points."
  }
]